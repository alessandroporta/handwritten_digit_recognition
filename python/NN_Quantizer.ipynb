{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardobove/handwritten_digit_recognition/blob/verilog_hdl/python/NN_Quantizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dHhV3Jk1g00"
      },
      "source": [
        "## Load the Dataset\n",
        "MNIST is a dataset of 70,000 handwritten images. Keras provides a function to directly download the data.\n",
        "<br>The dataset is then split into 60K training images and 10K test images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVQWFQz1g04",
        "outputId": "60241fa5-6e81-4ff1-8372-ffbed931a494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() #load and split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH4zIJJf1g05"
      },
      "source": [
        "## Normalize the images\n",
        "The input are 28 pixels by 28 pixels images. They are represented in grayscale, which means each pixel is a single number between 0 and 255. 0 is a black pixel and 255 is a white pixel.\n",
        "<br>Before training, it is a good practice to normalize the data. In our case the min-max scaler is used to get values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mbyWnwk41g06"
      },
      "outputs": [],
      "source": [
        "# Convert datasets to float\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "\n",
        "# Normalize the pixel data\n",
        "x_train /= 255.0\n",
        "x_test  /= 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8vm9a6Z1g07"
      },
      "source": [
        "## Construct the NN model\n",
        "\n",
        "Our NN will have the following structure\n",
        "1. **AveragePooling:** This layer turns a 28x28 image to 14x14. It takes the input image and transforms each 4x4 sub-matrix by replacing it with its average value.\n",
        "2. **Flatten:** This layer converts a 14x14 matrix to a 196x1 1D vector. This conversion is needed for the next layers.\n",
        "3. **Dense layer 1:** In a dense layer (also known as a fully connected layer), the output of each neuron is calculated by a weighted sum of the inputs from all neurons in the preceding layer. This sum is passed through an activation function before being propagated to the next layer.\n",
        "4. **Dense layer 2:** The output layer has 10 neurons: one for each of the possible digit. The neuron with the largest value corresponds to the recognized digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "AAKDycnQ1g08",
        "outputId": "946b83e1-11f8-4b74-e132-9eb0918b10b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/pooling/base_pooling.py:23: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(name=name, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ average_pooling2d (\u001b[38;5;33mAveragePooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m6,304\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m330\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ average_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,304</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,634\u001b[0m (25.91 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,634</span> (25.91 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,634\u001b[0m (25.91 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,634</span> (25.91 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Importing Keras model and layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, AveragePooling2D\n",
        "\n",
        "# Construct the NN by stacking all required layers\n",
        "model_s_nn = Sequential() # Sequential: the layers will be connected to one another\n",
        "model_s_nn.add(AveragePooling2D(pool_size=(2, 2), input_shape=(28, 28, 1)))\n",
        "model_s_nn.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model_s_nn.add(Dense(32, activation=tf.nn.relu))\n",
        "model_s_nn.add(Dense(10,activation=tf.nn.softmax))\n",
        "model_s_nn.summary() # Print NN summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL54joSa1g0_"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "The model can now be trained using the training dataset. For each image x_train(i), the model will try to find the associated digit y_train(i). It will gradually adjust the values for the different weights and biases of the dense layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8NofiF01g1A",
        "outputId": "738655f0-451e-40aa-84e0-29f83438405e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7624 - loss: 0.8648\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9208 - loss: 0.2745\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9371 - loss: 0.2199\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9425 - loss: 0.1997\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9490 - loss: 0.1753\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9537 - loss: 0.1566\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9576 - loss: 0.1440\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9606 - loss: 0.1323\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9637 - loss: 0.1209\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9679 - loss: 0.1104\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ee9b9f39290>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Train the model\n",
        "model_s_nn.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']) # Train on accuracy\n",
        "model_s_nn.fit(x=x_train,y=y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6hdWkdz1g1A"
      },
      "source": [
        "## Test the model\n",
        "The accuracy of the model on the training set is approx 97%, which is pretty good.\n",
        "<br>To ensure that the model will actually perform well on new data and has not just 'learned' the training set, we need to test it on new data.\n",
        "Note that 'learning' the dataset is known as overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQV89b7b1g1B",
        "outputId": "ae3a3dd4-56a8-4344-e5c3-3c9ea70fb280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9572 - loss: 0.1404\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1214149072766304, 0.9643999934196472]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Test the trained model\n",
        "model_s_nn.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaZLoZpk1g1C"
      },
      "source": [
        "As we can see, our model performs well on unseen data. We will now turn the biases and weights to Verilog vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na0-7ZVb1g1C"
      },
      "source": [
        "## Explore the weights used by the model and Quantize\n",
        "The trained model has a set of weights and biases for each neuron. The output of a neuron is being calculated as: out(n)=Sum(wn(k)*in(k))+b(n).\n",
        "<br>A 5 neurons layer which takes into input a vector of 20 elements has:\n",
        "* 5 biases, one per neuron\n",
        "* 20*5 weights, 20 per neuron\n",
        "<br><br>\n",
        "Those biases and neurons are coded as floats, which would take too much space. We will reduce these to 8 bits so that it can be stored into our board.\n",
        "<br> The verilog arrays generated are indexed as follows:  array_layer[neuron_num][weight_num]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SOYht_T-1g1D"
      },
      "outputs": [],
      "source": [
        "# Define some helper functions that will quantize an array to N bits / print the result\n",
        "\n",
        "# Returns a quantized array\n",
        "# Arguments:\n",
        "# use_scale       If non-zero it will use this instead of auto-computing the scaling factor.\n",
        "# Returns:\n",
        "# out             The quantized data\n",
        "# out_int         The quantized data, but scaled to an int value in the range +/- (2**n_bit-1)-1\n",
        "# scale           The scaling factor used between out and out_int\n",
        "\n",
        "def quantize_nbit(data, n_bit, use_scale=0, verbose=0):\n",
        "    max_bit_val = (2**(n_bit-1))-1\n",
        "    max_val     = np.max(np.abs(data))\n",
        "    if use_scale > 0:\n",
        "        scale = use_scale\n",
        "    else :\n",
        "        scale   = max_bit_val / max_val\n",
        "    if verbose:\n",
        "        print('Quantizing to +/- {}, scaling by {}'.format(max_bit_val, scale))\n",
        "\n",
        "    out_int = np.around(data * scale)\n",
        "    out = out_int /  scale\n",
        "\n",
        "    return out, out_int, scale\n",
        "\n",
        "import IPython.display as dp\n",
        "def print_nowrap(s):\n",
        "    display(dp.HTML(\"<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>\" +s+ \"</span>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "F-Xj6ezj1g1D",
        "outputId": "85162421-52a4-482f-e611-c3e3094f18e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<AveragePooling2D name=average_pooling2d, built=True>, <Flatten name=flatten, built=True>, <Dense name=dense, built=True>, <Dense name=dense_1, built=True>]\n",
            "Layer 2 - Array Shape/Range: Weights = (196, 32), Biases = (32,)\n",
            "Quantizing to +/- 127, scaling by 82.98216895503383\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>localparam signed B_ARRAY_L2 [8*32-1:0] = { -20, 20, 13, 8, 9, 9, 7, 7, -4, 4, -7, 16, 33, 25, 21, 7, 31, 19, -8, 20, 20, 42, -8, -23, 9, -7, -7, 32, 12, -3, -10, 0 };</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>localparam signed W_ARRAY_L2 [8*32*196-1:0] = {<br>10, 12, -4, -18, -11, 13, -5, 22, 34, -25, -7, -10, -1, -8, 0, 15, 4, 0, 6, 5, 65, 61, 56, 17, -18, -45, 2, -21, 15, 11, -25, -26, -14, 27, 26, 78, 63, 25, -5, -49, -72, -11, 34, -5, -43, -27, -7, -26, -25, 15, 63, 57, 35, 28, -45, -30, 45, -16, -51, -6, -22, -35, -48, -8, 57, 71, 40, 42, -5, -9, 30, 10, -37, -47, -31, -16, -41, 0, -8, 23, 60, 50, -12, -28, -18, -14, -9, -17, 3, -1, -34, -12, 2, 11, 21, 11, -17, 10, -5, -11, 19, 7, -4, -2, -6, 1, -6, -12, -5, -15, -21, 53, 47, 14, 25, 6, -2, 1, -19, -27, 6, -30, -55, -30, -23, 9, 42, -14, -7, 3, 19, -3, -16, -1, 0, -15, -34, -10, 26, 44, 35, -14, 20, 30, 8, 3, 25, -12, 0, -6, -12, -12, 36, 13, 0, 15, 22, 42, 23, 14, 16, 16, 3, -13, -49, -14, -13, 17, 3, 58, 29, 18, 33, 21, 34, 8, 6, -17, 14, -7, -27, 4, -7, 23, 7, 30, 59, 56, 28, 39, 25, 56, 27, -27, -16, 7,<br>4, 5, -2, -16, -5, 8, 32, 40, 62, 50, 42, 36, 4, 10, -7, 35, 28, -15, -3, -5, 5, -9, -33, -40, -52, -19, -25, -6, 1, 3, 7, -3, 0, -12, 2, -24, -35, -35, -26, -25, -5, 13, -31, 10, -29, -5, 1, 5, 2, 14, 1, -26, -12, -4, -16, -2, 35, -10, 13, -2, -8, -7, 22, 54, 11, -36, -54, -33, -15, -8, 24, 6, 34, 19, 13, 27, 8, 38, 44, 14, -56, -61, -35, -18, -16, 9, 13, 15, 11, -17, -24, 45, 55, 42, -20, -37, -14, 5, -9, 28, -55, -44, -38, -31, 10, 48, 27, -1, -60, -54, 17, 38, 13, 25, -46, -82, -43, 30, 28, 33, -15, -46, -36, -41, 0, 64, 23, 9, -95, -83, -23, 47, 33, 11, -30, -7, -26, -19, 17, 74, 2, -38, -59, -37, -18, 8, 9, 5, -3, 9, 4, -18, 19, 61, 5, -12, -24, -19, 6, -3, 0, 8, 2, 20, 20, 30, -4, -4, -2, -58, -32, -36, -38, -31, -26, -11, -3, 18, 21, -2, 23, 19, -6, 21, -23, 8, 26, -4, 6, 20, 11, 31, 41, 24, 1, -13,<br>-13, 9, -19, -6, 8, -41, -35, 37, 46, -45, -43, -16, -3, -5, 5, 30, 5, 24, 39, 30, 40, 39, 22, 5, -17, -33, -20, -11, -1, 35, 23, -15, 7, 26, 20, 22, 12, 15, 15, 1, -11, -31, 38, 4, -8, -4, 0, 12, 21, 39, 15, 6, -4, 8, -57, -40, 47, -13, 4, -8, -20, -34, -33, 13, 32, 7, -1, 18, -14, -6, 37, -12, -37, -39, -62, -99, -127, -8, 4, -7, -9, -2, 36, -9, -23, -9, -65, -81, -67, -58, -23, 4, 1, 2, -21, -47, -29, 23, -32, 6, -34, -3, 5, 1, 20, 15, -3, -3, -10, -53, -17, 54, -3, 41, 34, 8, -12, -3, 3, 9, -1, -24, -34, -44, -4, 72, -17, 46, 31, 22, 37, 13, 38, 26, 6, -1, 1, -11, 27, 50, -23, 57, 34, 37, 33, 30, 42, 45, 20, 25, 43, 41, 64, 40, 5, 13, 44, 2, 7, 19, -3, -2, -12, 9, 22, 32, 48, 16, -8, -19, 0, -23, -10, -9, 29, -8, -5, -23, -45, -14, 44, 25, 7, -14, -21, -27, -10, 13, -8, -1, -6, -11, 5, -14, -2, 9,<br>1, -6, -30, -39, -56, -79, -96, -39, -37, -61, -49, -30, 10, -6, 8, -36, 13, -46, 0, -9, -22, -44, -28, -40, -49, -55, 8, -13, -10, 32, -12, -17, 3, -2, -9, -8, -3, -5, -2, -16, 2, -25, -34, -1, -20, -22, -26, -25, -26, -14, 5, 14, 4, -7, -8, 22, -27, -20, -68, -43, -23, 0, 10, -7, -1, 6, -1, -13, 9, 9, -50, -12, -22, -16, -1, 8, 4, -6, 3, 1, -18, -49, -33, -28, -14, -24, -46, -9, -9, 6, 34, 12, -9, 7, -10, -50, 4, 40, -8, 49, 34, 26, 23, 19, 33, 5, 49, 16, -31, -2, 54, 69, -2, 27, 43, 45, 39, 33, 28, 34, 64, 10, 8, 23, 13, 63, -41, 36, 34, 8, 27, 30, 12, 43, 18, 14, 6, 19, 18, 21, -4, 68, 3, -34, -26, -36, -50, -49, -26, -20, -9, -11, 33, 4, 0, 41, -24, -18, -37, -47, -56, -48, -38, -32, -28, 8, 25, -8, 2, -23, 14, -14, 0, -19, -10, -11, -13, 10, 34, 27, 43, 17, 5, 20, 39, 33, 1, 5, 21, 37, 22, -6, 12, -1, -4, -2,<br>-6, 0, -1, -17, 1, -10, -14, -1, -16, 3, -8, 9, -12, -5, 12, 6, 21, -5, -37, -11, 5, -16, -1, 1, -9, -34, -5, 7, -4, 38, 43, 9, -11, -3, -8, 9, -5, -2, -39, -35, -20, -4, 47, 9, 23, -1, 15, 15, 14, 9, 18, 7, -19, -9, 6, 47, -13, 31, 8, -1, -4, -17, -17, 19, 0, -21, -6, -9, 0, -13, 9, 35, -16, -17, -40, -30, 15, 11, -7, -20, -10, 9, 24, -1, -2, -27, -4, -12, 1, 44, 46, 16, 12, -6, -13, 1, 20, 29, 4, 3, -31, -45, 23, 69, 35, 23, 26, 6, -1, 15, -36, -1, 15, -9, -74, -127, -109, -24, 3, -2, -8, -11, -13, -5, -60, -23, -20, 49, -20, -49, -103, -108, -57, -16, -9, -19, -9, 1, -42, 19, 42, 52, 28, 32, 30, 14, 9, -5, -22, -10, -2, -9, -17, 12, 7, 59, 64, 5, 28, 39, 8, -4, -18, -1, 1, -9, -18, 22, 4, 50, 91, 37, 26, 7, -2, -14, -28, -23, -16, 4, 30, 31, -4, 21, 5, -10, -29, -11, -10, -1, 11, 39, 56, -7, -9, 11,<br>7, -5, 10, -4, -8, -31, -43, -24, -28, -28, -5, 10, 1, 9, -13, 5, -12, 4, 12, -15, -29, -25, -36, -11, 7, 18, 16, 2, -15, -43, -27, -16, -9, 1, -6, -11, 5, -5, 6, 40, 49, 29, 25, -5, 12, 2, 17, -10, -11, -4, -1, 32, 28, 23, 48, 23, 43, 15, 27, 20, 20, 37, 47, 24, 7, 9, 7, 36, 12, 50, 41, 13, 60, 42, 22, 33, 33, -2, -34, -43, -50, -79, -63, 53, 37, -4, 11, 0, -8, -47, -61, -43, -20, -13, -23, -70, -94, 22, 15, -45, -81, -60, -48, -54, -28, 1, -26, 1, -28, -10, -35, 24, -30, -22, -99, -91, -48, -16, 44, 5, -10, -10, 2, 4, 41, 65, -15, 1, -9, -9, -5, -14, 17, 18, 23, 0, 3, 13, 23, 67, -21, 38, 27, 27, 30, 35, 30, 32, 22, 14, 11, 27, 18, 32, -14, -4, 45, 22, 21, 17, 8, 16, 17, 18, 16, 2, 29, 27, 13, 4, 11, 6, 28, 10, 28, 32, 26, 54, 1, 2, 29, 25, -12, -14, -11, 6, 20, 33, 22, 14, 15, 16, -1, -15, 11, -4,<br>2, -9, -14, -30, -13, -23, -31, -27, -42, -42, -48, -39, -5, -13, 8, -32, -25, -85, -105, -65, -40, -32, -46, -18, -1, -2, -53, -21, 10, -9, -8, -4, -46, -35, 2, -34, -24, 0, 12, 17, 8, -29, -9, -15, -8, 10, 10, 0, 19, 30, 25, 39, 33, 11, 0, 26, -31, -6, -20, 3, 17, 2, 9, 4, 13, 20, -7, 9, 21, 1, -29, -11, 0, 5, 9, -8, -38, -41, -1, -25, -9, -19, -39, -9, 12, -11, 10, 13, 7, 16, -21, 22, 5, -15, 1, 2, -87, -36, 8, 18, 11, 23, 43, 25, 3, 3, 34, 15, -7, -19, -50, -26, 22, -21, -25, 7, 40, 47, 37, 22, 23, 6, -1, -17, -59, -25, -24, -26, -28, -30, -27, -5, -13, 18, 14, 7, 18, -53, -90, -26, -8, -27, -60, -50, -51, -63, -31, -10, -1, 6, 0, -35, -12, -31, 9, 7, -8, -3, -3, -14, -30, -19, 5, 6, 21, -2, 17, 23, 5, 18, 33, 29, 22, 30, 13, -3, 2, 22, 26, 23, -32, -20, -1, 30, 47, 68, 56, 25, 34, 48, 40, -5, 22, 18, 24, 6,<br>-5, -1, -25, -31, -32, -37, -27, -23, -61, -48, -43, -36, -10, -13, -12, -42, -29, -41, -86, -53, -59, -40, -17, -43, -37, -7, -5, 3, 13, 2, 49, 22, -2, -18, -14, -14, 16, 31, 50, 43, 2, -61, 5, 51, 30, 11, 0, -11, -3, -40, 2, 48, 37, 28, -23, 0, 55, 39, 34, 5, 6, -7, -15, -78, 24, 61, 21, 4, -38, -46, 60, 30, -10, -7, 3, 7, -39, -104, 57, 54, 46, 5, -98, -103, 26, 9, -5, 23, 13, 19, -38, -61, 44, 19, 31, 9, -66, -30, -2, 30, 19, 30, 20, -23, -27, -21, 35, 47, 20, -3, -51, 10, 0, -6, -9, 4, 22, -15, -28, -2, 22, 25, 1, -24, -53, -11, 37, 18, -5, 3, 27, -6, -9, -10, 9, 1, -8, -34, -49, -15, 17, 22, 7, -8, 15, 29, -14, -14, 2, -6, 4, 11, -17, -19, 8, -58, -8, 33, 15, 24, -2, -4, 0, 2, 23, 8, 9, -8, -4, -36, -10, 13, 12, 7, -3, -21, -21, -45, -23, -36, -41, -9, 9, -18, 1, 12, 14, -17, -12, -8, -5, -25, -22, 10, 3, 6,<br>3, 2, 21, 27, 42, 71, 57, -4, 62, 61, 44, 40, 3, -3, 7, 18, 35, 65, 43, 60, 30, 10, 11, 29, 40, 40, 14, -1, -1, 7, -20, -34, 3, 25, 29, -10, 16, 16, 6, -9, -27, 20, 2, -33, -13, -18, -6, 7, 11, 26, -12, -10, 12, -16, -12, -2, -20, -45, -8, -15, -5, -3, -22, -5, -14, -34, -28, -42, -69, -16, -53, -31, -16, 10, 31, 9, -64, -47, -12, -9, -26, -48, -64, -35, -47, -60, -9, 22, 25, 27, -7, 9, 27, 25, 31, 32, -12, -44, 2, -48, -19, 9, 11, 11, 29, 31, 19, 8, 23, 12, -1, -6, -10, -10, -35, -5, -10, 27, 18, -26, -5, 27, 15, -5, -8, -15, -37, 15, 13, 16, 0, -4, -33, -20, -3, 16, 1, 10, -2, -11, -16, 20, 9, -9, -19, -13, -4, 12, -7, -1, 6, 13, 13, -4, -5, 32, 2, -5, -1, 15, 18, 20, 4, -6, 12, 31, 40, -5, -5, 16, 19, 28, 30, 27, 23, 18, 29, 22, 34, 47, 83, -8, -8, 9, 55, 68, 74, 51, 52, 21, 10, -2, 14, -22, 4, 13,<br>13, -8, 9, 13, -2, -47, -54, 4, 3, -19, -20, 27, -3, -4, -8, 42, 42, 57, 42, 41, 41, 32, 17, 17, 0, -1, 18, -15, -5, 64, 38, 22, -2, 40, 26, 32, 34, 35, 32, -11, -3, 9, -11, -19, 0, -4, 9, 4, -3, 13, 0, -19, -20, -9, 15, 11, -64, -12, -4, -1, -21, -6, 18, -5, -18, -10, 2, -1, 2, 41, -42, -9, -2, -19, -21, 6, 6, 16, -12, 5, -13, -16, 2, 52, -61, -8, -4, -12, 6, -20, 27, 36, 19, 11, -12, -63, -41, 41, -23, -10, -23, -6, -6, -40, 29, 51, 14, -33, -48, -64, -31, 28, 10, 30, 9, 2, -15, -15, 16, -2, -14, -18, -6, -19, 21, 21, -35, -13, 25, 16, 18, -17, 5, 25, 42, 17, 19, 21, 30, 11, -8, 19, 16, 41, 25, -18, 9, 35, 42, 33, 44, 47, 45, -15, 22, 48, 26, -3, 5, -5, -2, -10, 16, 27, 39, 48, 39, -33, 11, -13, -39, -14, 15, -19, 6, 5, 12, 44, 34, 29, 63, 28, 1, 16, 3, -33, 16, 18, 14, 29, 13, 5, -2, -12, 2, 3,<br>13, -3, 23, 31, 41, 67, 73, -16, 33, 64, 53, 23, -9, 4, 7, 12, 31, 48, 65, 77, 39, 30, 30, 40, 36, 48, 7, 18, 8, 23, 14, 34, 21, 13, -13, 4, 47, 21, 57, 46, 29, 46, 10, -23, 45, -1, 10, -17, -6, -3, -5, 2, 10, 55, 75, 33, -28, -2, 15, -12, 5, -4, 18, 3, 18, 16, 25, 29, 50, 9, -27, 7, -6, -1, -11, -4, -1, -17, -2, -8, -6, 4, 49, 9, -24, -20, -15, 6, 4, 20, 28, -5, -6, -7, -25, -12, 67, -26, 6, -14, 0, 1, 5, 24, 43, 23, -10, -2, -12, -2, 43, -24, 20, 33, 2, 5, 27, 54, 25, 10, -2, -8, 11, 21, 10, -32, -42, 28, 25, 34, 18, 40, 5, 7, 3, -16, 18, 39, -22, -60, 29, 21, 1, 21, 13, 14, 2, 34, 11, 21, 44, 19, -68, 1, -1, 63, -8, -21, -4, 23, 42, 35, 29, 23, 10, 21, -48, -8, 4, 7, -22, -4, 13, 21, 23, 25, 24, 1, -29, 0, 4, -6, 3, 10, -18, -43, -56, -17, -30, -58, -34, -39, -68, -60, -13, -9,<br>13, -10, 11, -4, 3, 3, 31, 17, -8, 57, 48, 12, -3, 10, -1, 42, 7, -24, -20, -19, -35, 3, 19, 31, 44, 38, -18, 12, 2, 27, -19, -19, -19, -14, -13, 6, 10, 32, 43, 42, 50, 65, 4, -33, -35, -55, -37, -34, -1, 7, 2, 6, 17, 22, 78, 73, -59, -63, -63, -18, -21, 5, 14, 10, -26, -10, -9, -6, 58, 28, -26, -14, -14, -36, -44, 0, 39, 38, -1, -6, -37, -16, 30, 42, -42, -13, -27, -57, -65, -1, 53, 38, 4, -10, -42, -100, -91, -10, 20, -14, -17, -35, -47, -40, 11, -6, -16, 0, -4, -70, -53, -50, 11, 4, 18, 9, -43, -37, -22, -10, 8, 43, 14, -48, -76, -55, -43, 27, 46, 41, 2, -6, -1, 10, 31, 38, -8, -66, -36, -34, -35, -3, -1, 4, 19, 27, 8, 20, 32, 20, -31, -78, -37, 8, 0, 8, -14, -4, 11, 17, 15, 29, 27, -22, -67, -77, -29, 27, 10, 49, -11, 4, 23, 8, 1, -35, -35, -49, -43, -34, -24, 30, -10, 16, 0, -49, -40, -29, -69, -45, -14, -10, 2, -14, -1, 13,<br>1, 2, 22, 43, 42, 58, 43, -11, 55, 38, 41, 27, -10, -2, -10, 18, 9, 18, 43, 58, 41, 38, 25, 27, 13, 29, 34, 8, 1, -6, -24, -17, 2, -6, -8, -4, 22, 32, 29, 39, 29, 19, -16, -27, 22, 18, -3, -20, -4, -29, -44, -30, -8, 32, 68, 48, -3, -10, -4, -4, -19, 10, 1, -24, -11, 1, 22, 1, 46, -18, 21, -31, -18, 0, -15, -21, -13, -37, -5, 12, -6, -9, 40, 1, 6, -19, -8, -10, -22, 12, 27, -39, -44, -9, -9, -18, 30, 14, 14, 7, -1, -1, -1, 37, 31, -4, -14, -13, -3, 9, 49, 44, -35, 38, 23, 6, -5, 28, 22, -1, 6, 7, -12, 6, 33, 35, -48, 35, 35, 31, 0, 16, 23, 2, 9, -15, -8, -3, 16, -1, -8, 54, 19, 17, 37, 53, 46, 27, 34, 25, 17, 26, -11, 24, -22, 47, 13, -19, 8, 22, 14, 19, 18, 27, 2, 24, 2, -10, -5, -2, -34, -52, -43, -25, -10, 1, -13, -7, -11, 10, 42, -6, 11, -3, -37, -74, -92, -32, -16, -50, -26, -54, -63, -21, -25, 8,<br>7, -11, -12, -40, -59, -67, -76, -14, -87, -65, -48, -26, 2, -5, 10, -23, -8, -39, -52, -37, -49, -40, -3, 12, 17, -9, -4, -15, 4, -4, -2, -3, -1, -6, 6, -17, -21, -17, 3, 4, 12, 6, 36, 28, 5, 18, 34, 21, 21, 4, 14, 16, 3, 19, 32, 4, 25, 15, 4, 28, 28, 25, 25, 13, 26, 24, 39, 34, 84, 49, 37, 57, 27, 28, 36, 23, 34, 67, 55, 36, 15, 9, 51, 97, 46, 35, 29, 12, -3, 13, 9, 36, 57, 33, -3, -51, -71, 29, 49, -8, 6, -28, -32, -13, -17, -16, 12, 37, 5, -15, -20, -31, 23, 9, 17, -1, -37, -26, -24, -15, 20, 32, -11, 2, -33, -40, 43, 20, 44, 0, -6, 5, -10, 2, -1, -6, 3, -24, -2, -37, 2, -39, -6, 1, -12, -14, -4, -4, -15, -11, 0, -18, 18, -28, -14, -18, -27, -14, -22, -10, -13, 1, -8, -12, -39, -8, -3, 18, 0, 61, 49, 37, 43, 41, 55, 51, 44, 21, 1, 8, -4, -36, 0, -28, -15, 18, 63, 73, 83, 81, 98, 121, 55, 64, -5, -13,<br>-5, 0, -5, 5, -4, -9, -3, -18, 0, -2, -3, -1, 7, 11, -11, 1, -19, -11, -9, -12, -4, -4, -10, -13, -20, -3, 34, 0, 2, -24, -25, 3, 1, 9, -13, -4, 10, 23, 6, -10, -24, -12, -3, -16, -10, 2, 4, 35, 42, 7, 12, 24, 10, 17, -27, -44, 43, -16, 13, -12, 11, 32, 48, 34, 23, 36, 14, 38, 48, -57, 50, -32, -13, -26, -20, -9, 2, -46, -8, 5, 13, 4, 40, -5, 28, -25, -40, -13, -11, -12, -13, -53, -37, -26, -29, -12, 15, -10, 4, -10, -18, -4, -4, 12, -26, -49, -47, -48, -29, -18, 24, 63, -23, -24, 3, -5, 18, 20, -31, -43, -13, -16, 14, 1, 25, 67, 1, -23, -5, 6, 3, -4, -19, -17, 6, 17, 6, 15, 5, 39, -2, 25, -14, 26, 11, 13, 26, 15, -4, -2, 6, 11, -20, 18, -17, -16, -4, 31, 53, 40, 34, 17, 6, -8, 5, 31, 18, -22, 2, -29, 2, 8, 22, 6, 29, 37, 44, 34, 0, 16, 61, 10, -2, 0, -33, -9, 4, -4, -7, 34, 26, 25, -14, -14, 23, 1,<br>-9, 13, 13, 45, 41, 89, 91, 27, 35, 68, 56, 29, -9, -9, 11, 35, 25, 64, 70, 78, 61, 37, 8, 8, 4, 1, -18, 3, -11, 8, 41, 6, 6, 9, -16, -9, -23, -6, -13, -7, -37, 0, 36, 54, 38, -14, -30, -39, -9, 7, -30, -43, -27, -5, 1, -14, 9, 23, 43, -19, -33, -46, -21, 20, 6, -34, -5, -39, -52, -47, 20, -11, -13, -8, -30, -16, 22, 20, 23, -4, 0, 2, -29, -84, -35, -58, -14, 20, 40, 50, 30, -16, 12, -10, -18, 27, 19, -54, -8, -7, 17, 26, 48, 8, -27, -16, -3, 11, 18, 28, 51, -48, 34, 33, 38, 0, -8, -4, -13, 1, 27, 37, 20, 23, 10, -26, -17, 6, -4, 1, 11, 30, 41, 38, 40, 11, -2, 7, -22, -18, 28, 27, -18, -10, 17, 42, 47, 38, -4, 1, -11, -60, -66, 20, 10, 44, 6, -7, 21, 10, -6, -18, -14, -25, -55, -54, -72, 6, 3, 45, -12, -29, -61, -75, -87, -84, -90, -40, -20, -21, 1, 21, -7, -14, -52, -76, -102, -92, -92, -90, -60, -102, -71, -40, 10, 13,<br>-11, -4, 18, 45, 58, 62, 90, -3, 9, 80, 58, 43, -7, 0, -2, 10, 12, 27, 7, 34, 19, 33, 9, 32, 32, 40, -9, -15, 6, -30, -19, -3, 24, 7, 4, 6, 18, 12, 30, 26, -16, -56, -3, -47, 7, 24, 29, 21, -26, -20, 15, 7, -9, 4, 11, 8, 0, -36, -6, 12, 39, 18, -60, -64, -32, -47, -44, -51, -26, -20, 16, -78, -16, 1, 56, 10, -66, -47, -9, -17, -21, -8, -17, -61, 25, -59, -28, 27, 20, 25, -26, -19, 27, 17, 23, 29, -25, -107, 43, -13, 17, 11, 17, 17, -7, -21, 14, 25, 1, 21, 3, -50, -13, -35, -16, 19, 12, 21, 0, 10, 34, 15, 18, -6, 3, -43, -16, 1, -17, -14, -10, 14, 28, 39, 15, -4, -3, -29, -6, -35, 19, 8, -28, -55, -33, 6, 33, 38, 10, -8, -12, -19, -17, 7, -12, -6, -32, -63, -42, -11, 20, 33, 29, 26, 16, 1, -2, 27, 3, 4, 21, -32, -29, -33, -16, -4, 7, 7, 8, 19, 1, -6, -9, 12, -7, 6, 9, 7, -37, -61, -70, -41, -43, -29, -7, 8,<br>10, 6, -1, -31, -23, -36, -30, -10, -24, -33, -15, -38, -9, 2, 13, -3, -28, -78, -87, -99, -93, -47, -28, 29, 5, 3, -18, -18, -11, 0, 18, 18, -24, 13, -15, -21, 11, 31, 17, 16, 18, -13, 0, -28, 46, 24, 21, 16, -31, -35, -14, 2, -7, 17, 19, 42, -48, -7, 12, 32, 27, 7, -23, -55, 0, 7, 13, 40, 60, 53, -31, -35, -2, 4, 28, 27, -23, -67, -16, -7, -4, 12, 37, 64, -9, -39, -7, 33, 29, 45, 49, 11, -2, 21, 35, 12, -15, -5, 34, -11, -11, -5, 25, 22, 55, 47, 27, 0, -7, 0, -29, -38, 9, -4, -40, -44, -55, -28, 27, 30, -2, -31, -36, -13, -4, -46, -47, -1, -6, -56, -67, -38, -29, -18, -1, -29, -19, -5, 4, -51, -29, 7, 15, 18, -8, -6, -24, 1, 13, 5, 8, 33, -28, -26, -2, 18, 42, 19, 25, 26, 10, 8, 26, 3, 8, 12, -6, 36, -10, 35, -20, -14, 4, 33, 20, 12, -11, 6, 14, 14, 1, 15, 7, -31, -54, -92, -77, -72, -80, -114, -92, -89, -75, -49, -21, 11,<br>-8, 10, -13, -16, -38, -56, -65, -13, -16, -36, -27, -11, 4, -4, -8, -40, -38, -7, -23, -40, -17, -1, -31, -37, -30, -14, -34, 0, -10, -9, -40, -27, -51, -48, -69, -52, -13, -12, -19, 2, -19, -15, 40, 24, -11, -13, 5, -3, 18, 24, 17, -12, -7, 1, 21, 48, -34, -6, -11, 27, 41, 55, 26, 3, -29, 2, 9, 15, -12, 35, -55, 53, 37, 47, 23, -19, -52, -47, 14, 33, 43, 7, -48, -58, -31, -3, -9, -39, -51, -44, -29, 13, 40, 38, 43, -5, -23, 19, -22, 9, -21, -3, -16, 11, 26, 14, 13, 22, 24, 8, -26, 55, 28, 31, 2, -11, -2, 8, 35, 33, 24, 5, -3, 5, -22, 35, -18, 34, 16, -1, -43, -25, 11, 34, 11, -5, -21, -36, -30, -5, -6, 10, -2, 6, -10, -7, -7, 20, -7, -16, -31, -28, 7, -13, -18, 29, -14, -9, -11, -6, -18, -23, -13, -10, -5, 16, 4, -16, 6, 16, 18, 42, 4, 3, -12, 0, -17, -8, 15, 4, -4, 3, -12, 10, 54, 81, 58, 45, 41, 53, 18, 20, 28, -1, 14, 7,<br>5, 10, 9, 15, 43, 23, 23, 20, -19, 73, 56, 0, 4, 11, 7, -21, 5, 33, -20, -16, 2, 26, 24, 2, 8, 18, -11, 7, 1, -29, 11, -2, 11, -15, 10, 31, 15, 6, 12, -3, 3, 6, 10, -7, -9, -13, -16, -9, -20, -9, -11, -4, -12, 18, 23, 35, 14, 16, 7, 6, -22, -11, 1, -22, -33, -19, 0, 3, -18, 18, 42, -6, -2, -20, 10, 25, 38, 14, -12, -2, 8, 2, -38, -27, 38, 24, 7, 1, 34, 38, 12, 5, 38, 7, 17, 14, 10, -48, 2, 23, 12, 12, 22, 30, 8, 41, 48, -14, -27, -2, -12, -38, -25, 16, -14, 18, 20, -6, 25, 70, 25, -23, 10, 8, 6, -27, 25, 8, -3, -12, -5, -19, 14, 18, -8, -3, 17, -5, 15, 19, -14, -53, -31, 11, -4, 3, -6, 10, -8, 9, 2, -11, -24, -30, 18, -60, -14, 3, -6, -6, 0, 15, 8, 6, 9, -29, -41, -35, 5, -52, -38, -19, -16, -1, -6, -8, -17, -10, -22, -38, -66, 8, -3, -33, -45, -30, -30, -45, -43, -58, -69, -43, -48, -3, -20, 4,<br>-12, -6, 22, 44, 46, 93, 81, 19, 42, 57, 66, 28, -1, -2, -3, 39, 25, 31, 18, 5, 12, 18, 24, 39, 36, 66, -13, 26, 12, -22, -32, -11, -10, 4, -17, -18, -42, -18, 9, 3, 38, 80, -39, -26, -26, -8, 1, 12, 6, 8, -1, 6, 23, -5, 49, 3, -17, -25, -32, 7, 25, 24, 7, 12, -8, 3, 13, 17, 83, 20, -16, -34, -3, 42, 22, 19, 33, 15, -23, 13, 25, 53, 94, 70, -36, 30, 44, 31, 5, -4, 12, 37, -2, 5, 7, 47, 2, -53, 15, -22, -10, -6, 4, 12, 24, 10, -7, -9, -16, 11, -23, -87, -27, -59, -8, 6, 12, 17, 21, 2, -27, -9, -31, -13, -11, -71, 50, -73, -26, -3, 46, 49, 25, -3, -7, 9, -8, -5, 18, -24, 12, -62, 3, 5, 26, 24, 21, 8, -18, -6, 9, -18, 28, -17, 4, -31, 6, -5, -12, -20, -2, 8, -20, -35, -36, -23, 7, 19, 9, 2, -18, -14, -8, 2, 9, -16, -11, 23, 12, 20, -5, -7, -5, 24, 19, 19, 66, 49, 84, 78, 72, 69, 97, 46, 25, 9,<br>-1, 10, -2, -24, -20, -46, -51, -40, -31, -36, -42, -25, -3, 0, -2, -35, -38, -17, -48, -97, -99, -105, -66, -51, -59, -64, -61, -22, 3, -1, -33, -32, -43, -46, -49, -36, -42, -59, -47, -34, -42, -20, -8, 38, -4, -5, -4, -8, -5, -1, -10, -25, 12, 36, 8, 15, 11, 26, 11, 10, 3, -16, 35, 32, 38, 28, 9, 29, 53, 18, 21, 27, -54, -25, -21, 22, 46, 47, 41, 22, 5, 21, 66, 43, 33, 33, -37, 16, 36, 21, -1, 35, 24, -36, -42, -23, 7, 44, 16, 37, 7, 1, 31, 10, -25, 23, 29, -17, -24, -3, -16, -39, -5, -11, -1, -17, 11, -3, 19, 21, -3, -38, 3, -28, -51, -48, 33, -49, -10, -13, -22, -22, 10, 3, -40, -51, -36, -66, -76, 42, 0, -87, -48, -39, -18, -19, 4, -9, -43, -61, -51, -61, -41, -13, 9, -59, -22, -25, 16, 17, 5, -6, -15, -12, -25, -23, 7, 20, 10, -18, 6, -17, -19, -3, -27, -24, -12, -12, 9, 25, 13, 11, 12, -2, 5, 9, 34, 26, 22, 44, 1, 13, 48, 49, 10, 2,<br>-3, -5, 10, -14, -18, 0, -6, 18, -13, -4, -5, -8, -2, 6, -1, 16, 6, 14, 39, 23, 26, 3, -16, -20, 0, -11, 25, 4, 12, 1, -53, 1, 9, 33, 41, 33, 33, 30, 19, 5, 29, 28, -29, -36, -35, -8, 14, 49, 54, 81, 64, 39, 13, 2, -7, -2, -25, -28, -25, 17, 20, 10, 8, -1, 5, 6, -5, -8, 26, 2, -53, -36, 7, 15, 20, 2, -12, -44, -30, 9, -20, -15, 64, 70, -44, 16, -2, 5, -2, -23, -3, -5, -22, -1, 4, -29, -15, 37, -5, -26, -34, -25, -11, -5, 19, -28, -28, -21, -12, -25, -13, -13, -40, 3, -24, -26, -9, 21, 6, -41, -17, -35, -10, -11, -17, -38, 4, -29, -2, -6, -25, -11, -59, -43, -15, 11, 7, 14, 7, -51, -5, -10, 48, 31, -2, -4, -3, 8, 15, 18, 19, 17, 0, -14, -5, 46, 17, 40, 34, 42, 54, 35, 40, 2, 16, 55, 38, -13, -2, 22, -5, 6, 24, 43, 44, 44, 29, 31, 32, 23, 60, -35, -9, 5, 22, 13, 26, 62, 46, 30, 37, 6, 20, -10, 15, 2,<br>6, 9, 0, 24, 4, 0, 17, -31, 47, 1, 15, 39, -13, -11, -12, -5, -4, 24, 41, 71, 26, 16, 10, 11, 0, 6, 20, 8, -6, 4, 11, 8, 12, 17, 29, 7, 14, 8, -17, -40, -28, 23, 0, 46, 19, 16, 12, 12, 13, 16, 33, 12, -3, -20, -18, -14, -58, 43, 42, 12, -11, -24, 2, 4, 3, 5, 13, -3, -25, 4, -26, 36, 51, 7, 0, -8, -17, -22, -28, 22, 29, 37, -18, -16, -2, 42, 42, 7, -6, -4, -20, -16, -16, 10, 42, 82, 99, 11, -30, 7, -4, 24, -3, 12, 4, 4, 17, 32, 47, 11, 9, 33, -15, -15, -8, 16, 51, 28, 26, 6, 21, 4, 19, 8, 5, 39, -19, -13, -15, 8, 39, 20, 2, 8, 23, 0, 6, 12, -27, 8, -5, 8, 18, 18, 10, 1, 8, 10, 5, -4, -16, 1, -40, 0, -17, 32, -5, 13, 11, 14, 12, -3, -15, -18, -9, 28, -14, -38, -2, -28, -24, 13, -5, 11, -8, 5, -14, -13, 13, 3, -9, -2, 7, 31, 26, 45, 32, 20, 26, 15, -8, -15, 26, -12, 6, 13,<br>5, -2, 3, 0, -1, 40, 26, -2, -1, 50, 38, 21, -1, 3, -3, 17, -7, -27, -63, -44, -65, -43, -31, 1, 1, -25, -25, -6, -4, 28, 23, -37, -63, -42, -15, -25, -14, -10, -25, -25, -23, -6, -46, 5, -15, -88, -79, -23, -5, 27, 23, 17, 22, -9, -20, -17, -49, -22, -33, -77, -17, -7, 9, 20, 0, -10, -6, -21, -54, -38, 7, -67, -70, -52, -14, -15, -3, 17, 16, -8, 4, -21, -72, -17, -2, -51, -76, -32, 11, -5, 43, 54, 26, 27, 17, -50, -113, -42, -9, -33, -48, -55, -8, 13, 57, 19, 17, 12, -26, -83, -85, -58, -12, -8, -1, -54, -9, 24, 16, 13, 32, 9, -68, -54, -32, -2, -21, -39, -29, -34, -13, 1, 9, 13, 26, -35, -117, -32, -3, 1, 12, -25, 13, -1, -2, -10, 4, -9, -22, -48, -110, -34, 17, 23, 3, -8, -4, 20, 22, 17, 1, -8, -28, -62, -93, -37, 43, 9, 7, -37, 1, -3, 2, -9, 0, 4, 17, -12, -55, 8, 5, -23, 6, 17, 54, 34, 24, 20, 24, 46, 60, 43, 37, 16, -20, 9,<br>-1, -11, -11, -31, -56, -70, -64, -54, -7, -50, -34, 10, 11, 11, -12, -14, 8, -8, 15, 35, 18, 8, 8, 0, 21, -12, 5, -8, 16, 29, 42, 46, 41, 28, 5, 18, 14, -10, -15, -40, 7, 30, 56, 78, 66, 36, 26, 12, -4, -1, -14, -1, 11, -27, -53, 9, 44, 70, 46, 25, 6, -10, -6, 11, 11, 30, 16, 15, -79, 23, 23, 80, 18, -25, -25, -8, 22, 15, 21, 19, 9, -19, -42, -27, 27, 15, -53, -57, -14, 19, 9, 11, 18, 7, -40, -71, -6, -20, 28, 25, -23, -36, -22, -35, -12, 15, 25, 23, -8, 16, 38, 32, 45, 36, -7, -34, -39, -43, 9, 13, 29, 34, 31, 63, 38, 10, 42, 40, 25, -4, -17, -4, -13, -4, 14, -17, 4, 27, -28, 8, 7, 7, -14, -10, -4, 5, 0, 3, -13, -25, -23, -14, -35, -29, 10, 10, -21, 9, 3, 18, 17, 25, 0, 10, -19, -22, -38, -6, -13, 29, 38, 42, 20, 51, 46, 38, 27, -5, -37, -25, -1, -31, -5, -17, -33, -5, 20, 14, 0, -1, 19, 33, -14, -36, -1, -1,<br>12, 5, -18, -44, -45, -36, -43, -17, -90, -48, -49, -43, 3, 8, -1, -38, -52, -67, -83, -95, -77, -29, -24, -34, -16, 25, 28, 10, -3, -13, -62, -28, -36, -6, 19, 38, 32, 9, 15, 3, 31, -3, -46, -42, -16, -16, -17, -8, 3, 21, 17, 20, 4, 47, 11, -1, -54, -19, -15, 3, 5, 7, 13, -10, 1, 25, 28, 39, 77, 15, 10, -33, 3, 34, 41, 13, 13, -7, -14, 10, 6, 19, 28, 12, 16, -21, 4, 20, 20, 32, 13, -24, 1, 23, 60, 37, 22, -8, 0, -4, -9, 11, 14, 26, 15, -11, 1, 24, 0, 8, -4, -12, -27, -6, -8, 24, 28, 11, 5, 15, 35, -1, -6, 1, -9, -26, -29, -8, 16, 36, 21, 7, -2, 5, -1, -25, -5, -1, -41, -41, 13, 3, 22, 11, 16, -17, -13, -16, -5, -24, -3, 12, -61, -24, -4, -5, -23, 7, 18, 22, 17, 1, -8, -9, 4, 16, 1, -23, 0, -2, -57, -27, -23, 10, 17, 23, -2, 7, -7, -15, -56, -37, -8, 8, 5, -26, -46, -68, -66, -86, -87, -68, -68, -52, -16, 0,<br>9, -10, 21, 40, 53, 39, 37, -10, 29, 63, 58, 37, -11, 6, 2, 4, 30, 31, 30, 19, -6, -5, 6, 31, 21, 57, 21, 33, -7, -25, -34, -28, -14, -28, -29, -12, 8, -7, -9, -5, 54, 38, -37, -9, -27, -16, -1, -7, 18, 18, 13, 10, 0, -17, 56, 24, 14, -30, 6, -3, -14, 1, -4, -9, -28, -2, 3, -1, 73, 16, 2, -26, 33, 47, 8, 33, 9, -36, -18, -9, 19, 8, 58, 38, 39, 18, 44, 33, 16, 29, 10, -16, 15, -1, 29, 17, -26, -14, 2, 2, 6, 7, -15, 22, 12, 5, 24, 24, 15, -6, -1, -46, -25, -23, -7, 6, -15, 9, -33, 12, 44, 20, -7, -25, -13, 5, 36, -32, 5, 17, -13, 12, -11, 15, 15, -8, -21, -8, 1, -13, -23, -14, -3, -12, 20, 18, 17, -15, -24, -46, -52, -10, 20, -31, -13, -13, 8, 3, 15, 25, 5, -16, -18, -51, -38, 2, 60, -24, -8, -9, -15, -1, -5, -1, -5, 23, 26, 32, 22, 32, 69, -28, 6, 22, 10, 18, 44, 56, 91, 88, 73, 65, 85, 66, 19, 8,<br>1, 6, 12, -17, 14, 19, 21, 30, -24, -27, -15, -24, -10, -3, -3, 30, 10, -21, -5, -4, 36, 47, 51, 25, 37, -28, 51, 11, 14, 24, 32, 29, 20, 11, 22, 24, 7, 16, 27, 22, 7, 50, 27, 17, 20, 14, -9, 5, 11, 22, -2, 7, 1, 10, 18, -9, 69, 1, -5, 9, 5, 24, 41, 28, -23, -22, -7, -4, 16, -34, 53, 45, -7, -1, 10, 38, 45, -23, -7, 2, -16, 10, -8, 16, 11, 30, 16, 13, 23, 13, -43, -41, 2, -3, 9, 30, 20, -24, 23, 1, 16, 10, 0, -43, -43, -22, -22, -11, 43, 59, 48, -56, 26, -8, 10, 7, 5, -24, -52, -22, -22, 13, 28, 32, 6, -68, 48, 23, -19, -9, -10, -30, -93, 18, 36, 33, 32, 10, -14, -65, -24, -19, -15, 5, 1, -11, 3, 60, 62, 22, 17, -1, 36, -36, 11, -4, 5, -13, -3, 11, 22, 47, 36, 18, -7, -39, -4, 0, 10, 45, 10, -2, -19, 10, 0, 5, 18, -30, -26, -13, -12, 2, -12, -8, -21, 30, 68, 28, 40, 82, 66, 76, 59, 51, 12, 2,<br>-12, -12, -19, -15, -45, -12, -19, -11, 3, -46, -26, -6, -2, 9, 4, 19, -3, -16, 3, -10, -25, -41, -46, -65, -34, -47, -17, 5, -2, 29, 22, 26, 30, 40, 34, 0, -37, -38, -51, -42, -33, -6, 42, 73, 22, 35, 35, 37, 43, 36, 28, -12, -28, -31, -62, -60, 63, 35, 22, 12, 15, 27, 20, 42, 63, 38, -18, -35, -61, -41, 40, 26, -7, -1, -17, -8, 3, 20, 33, 19, -5, 4, -47, -41, 6, 12, -56, -26, -36, -19, -8, -21, -22, 1, -13, 5, 4, 24, -15, -4, -16, 1, -10, 6, -14, -18, -11, 26, -3, -2, 30, 38, 13, 9, -12, 1, 2, -20, -23, -30, 28, 27, 23, 14, 19, 47, 31, -13, -19, -26, -7, 8, -9, -21, 6, 1, -15, -7, 1, 8, 27, 25, -22, -36, -23, 5, -7, -6, -14, -36, -20, -12, -11, 20, -6, -24, -29, 3, -6, -5, -3, 4, 5, -31, -46, -3, -26, -47, -12, 1, 47, 15, -7, 16, 27, 11, 14, -11, -27, -35, -18, -1, 1, -25, -8, 10, 49, 25, 23, 38, 35, 41, 24, 13, 19, 2,<br>9, 11, -7, -4, 0, 3, 0, 18, 2, -2, -4, -9, -2, -11, -5, -6, 30, 39, 3, -9, -15, -26, -20, 6, 8, 14, 14, -2, 4, 41, 32, 44, 9, 24, 22, 19, 19, 11, 8, -26, 27, 33, -7, -6, 10, 12, 27, 2, 13, 7, -2, -16, -24, -21, 27, 4, -64, -7, 11, 14, 11, -4, -15, 4, 16, 14, 8, -14, -18, 10, -43, -11, -18, 3, -14, 1, 15, 30, 29, 36, -1, -33, -67, -10, -56, -42, -27, -4, 7, 8, 57, 52, 40, -18, -48, -30, -13, -17, -7, -23, -65, -51, -36, -15, 23, 36, -13, -41, -27, -11, -5, -19, 20, 9, -40, -37, -32, -48, -15, -19, -34, -26, -5, 28, -10, -41, -46, 16, -1, 13, 1, -39, -66, -27, 0, 31, 26, 4, -65, -50, 18, 9, 15, 22, -5, -23, -32, 8, 16, 20, 19, -8, -55, -38, 15, 28, -15, 15, -7, 19, 10, 6, 12, 15, 15, -16, -30, -14, 11, -12, -24, 7, 21, 20, 8, 9, 23, 8, -18, -3, -37, -30, -11, -2, -29, -30, 27, 9, -17, 3, 30, -6, -33, 0, -8, -7,<br>13, -11, 16, 32, 51, 51, 53, 28, 74, 49, 42, 18, 6, -3, 10, 39, 34, 71, 47, 37, 44, 25, 39, 20, 23, 16, 13, -9, 7, 13, 66, 24, 20, -27, -6, -10, 9, 0, -11, -9, -28, -2, 36, 35, 58, 39, 3, 8, 3, 0, 3, 2, 2, -5, -7, 21, 28, 5, 35, 25, 25, 17, -14, -4, 5, -3, -5, -16, -36, 26, 30, -17, -36, -5, -7, -45, -47, 28, -1, -16, -12, -30, -24, -5, -7, -18, -50, -94, -76, -57, 32, 48, -19, -21, 7, -6, 64, 32, -14, -13, -37, -29, -21, 23, 91, 42, -9, 17, 7, 26, 18, 34, 5, -1, 16, 19, 52, 65, 74, 10, -4, 16, 0, 5, 69, 47, -3, 24, -20, -7, 48, 23, 4, -7, 8, 1, -11, 22, 47, 88, -16, 27, -16, 18, 6, -27, -28, -16, -9, -18, -16, -28, -31, 29, 6, -7, 46, 27, -11, -4, 1, 2, -3, -12, -15, -33, -55, 0, 10, -13, -6, 0, -5, 13, -1, -18, -29, -52, -30, -38, 12, 25, -11, -33, -17, -36, -26, -7, -57, -41, -36, -36, -41, -14, 6, -8,<br>};<br></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 3 - Array Shape/Range: Weights = (32, 10), Biases = (10,)\n",
            "Quantizing to +/- 127, scaling by 57.36208002937688\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>localparam signed B_ARRAY_L3 [8*10-1:0] = { -21, 10, 3, -15, -4, 19, 2, 10, -2, -4 };</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>localparam signed W_ARRAY_L3 [8*10*32-1:0] = {<br>40, 32, -47, 10, -36, -65, -76, 6, -26, -66, 11, -36, 16, -5, 45, 28, 5, -75, -42, -43, 1, -35, 42, 12, 45, -47, 22, -12, 18, 39, -89, -40,<br>22, 42, 46, -21, 12, 22, -15, 62, -85, 23, -18, 2, -21, -17, -21, -20, -66, 20, -54, 25, 0, 10, -26, -57, 6, -47, -47, -93, 72, -66, 4, 42,<br>18, 49, 32, 60, -97, 58, -66, -5, 12, 18, -3, -65, 29, -36, 55, 11, -31, -34, 22, -6, -36, -42, 13, 28, -108, 6, -6, 19, -40, 24, -12, 28,<br>40, -33, 10, -9, 60, 2, 19, -27, 16, 21, 16, 6, -4, 18, -31, 26, -70, -1, 57, -27, -27, -52, 7, 7, -34, 35, -33, -38, -4, 13, 27, 5,<br>-114, -17, -48, 16, 26, -97, 28, 46, -4, -15, 6, -1, 12, -3, -9, -19, 26, 42, -9, 28, -48, 21, -120, 1, 10, -5, 31, -12, -16, -22, 26, -13,<br>-38, -127, 28, 27, 28, 53, 5, -84, 43, 5, 2, 34, 25, 31, -4, -13, 13, 27, -95, -28, 12, 51, 35, -37, -8, -26, -10, 26, 7, -36, -31, -40,<br>-61, -4, -50, -41, -28, -22, -34, -16, 30, -13, 19, 17, 33, -48, 19, 43, 50, -89, -2, 35, 9, -87, -8, 4, 22, -55, -22, 19, -20, -69, -65, 18,<br>4, 10, 27, -51, 8, 8, 6, 35, -42, -51, -63, -81, -73, 42, 27, -51, -5, -31, 7, 31, -11, 34, -48, -8, -43, 27, -10, 19, 37, 41, -59, 70,<br>8, 11, -55, -125, -121, -7, -15, -56, -6, 24, 20, 0, 0, 2, -12, -53, -7, 12, -20, 41, 17, -43, 20, 29, 3, -4, 30, 9, -88, -5, 41, -1,<br>33, 28, -93, 26, 27, -75, 25, 0, 26, 0, -30, -16, -54, 13, -73, -85, -12, -34, 19, -27, 12, 20, 21, 32, 50, -34, 0, 21, 4, 20, 25, -114,<br>};<br></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Quantize and Export the weights\n",
        "n_bit               = 8            # Number of bits to quantize to\n",
        "quantized_wb        = [0,0,0,0]\n",
        "quantized_wb_scale  = [0,0,0,0]\n",
        "\n",
        "print(model_s_nn.layers)\n",
        "\n",
        "for l,layer in enumerate(model_s_nn.layers):\n",
        "    if len(layer.weights)>0:\n",
        "        w,b = layer.weights\n",
        "        w,b = w.numpy(), b.numpy()\n",
        "        print('Layer {} - Array Shape/Range: Weights = {}, Biases = {}'.format(l, w.shape, b.shape))\n",
        "        w_q, w_qi, scale = quantize_nbit(w, n_bit, verbose=1)\n",
        "        b_q, b_qi, _     = quantize_nbit(b, n_bit, use_scale=scale)\n",
        "\n",
        "        # Print the scaled values\n",
        "        num_w, num_neuron = w.shape\n",
        "\n",
        "        # Print the Biases as an V parameter array\n",
        "        s = \"localparam signed B_ARRAY_L{} [{}*{}-1:0] = {{ {} }};\".format(l, n_bit, num_neuron, ', '.join(str(int(e)) for e in b_qi))\n",
        "        print_nowrap(s)\n",
        "\n",
        "        # Print Weights as an V parameter array\n",
        "        s = \"localparam signed W_ARRAY_L{} [{}*{}*{}-1:0] = {{<br>\".format(l, n_bit, num_neuron, len(w_qi[:,0]))\n",
        "        for n in range(num_neuron) :\n",
        "            # Note: you can change\n",
        "            #s += \"Layer {}, Neuron {}, Bias = {}, Weights = {}<br>\".format(l, n, int(b_qi[n]), ', '.join(str(int(e)) for e in w_qi[:,n]))\n",
        "            #s += \"bias_l{}[{}] = {}; weight_l{}[{}] = {{ {} }}<br>\".format(l, n, int(b_qi[n]), l, n, ', '.join(str(int(e)) for e in w_qi[:,n]))\n",
        "            s += \"{},<br>\".format(', '.join(str(int(e)) for e in w_qi[:,n]))\n",
        "        #s = s[0:-5] # remove last comma\n",
        "        s += \"};<br>\"\n",
        "        print_nowrap(s)\n",
        "\n",
        "\n",
        "        # Save the quantized weights/bias for use later\n",
        "        quantized_wb[l]       = (w_qi, b_qi)\n",
        "        quantized_wb_scale[l] = (w_q,  b_q)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bTRb7IW1g1E"
      },
      "source": [
        "The weights and biases are available above and can be added to the Verilog code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkPJ7iiW1g1E"
      },
      "source": [
        "## Creation of test vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Jr3xDB1g1F"
      },
      "source": [
        "This function transform images in test vectors to be used in the Verilog code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zE3XXP6j1g1F"
      },
      "outputs": [],
      "source": [
        "# Write data out in the form\n",
        "# logic signed [7:0] data [0:N-1] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 4, 0, 0, 0, 0, 53, 0, 0, 0, 0, 0, 0, 0, 82, 9, 0, 0, 0, 0, 95, 0, 0, 0, 0, 0, 0, 26, 98, 0, 0, 0, 0, 20, 102, 0, 0, 0, 0, 0, 0, 56, 73, 0, 0, 0, 0, 39, 86, 0, 0, 0, 0, 1, 32, 114, 40, 0, 0, 0, 0, 38, 103, 51, 63, 83, 91, 89, 55, 121, 4, 0, 0, 0, 0, 0, 36, 44, 44, 19, 0, 0, 33, 107, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 87, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 101, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
        "def test_data_to_verilog(n_bit, data, show_img=0, suffix=''):\n",
        "\n",
        "    # Input data - Flatten into 1-d vector\n",
        "    m_output = data.reshape(1, np.prod(data.shape))\n",
        "    m_output = m_output[0]\n",
        "\n",
        "    # check the output shape is 1D\n",
        "    if m_output.shape[0] != m_output.size:\n",
        "        print('Error: Model output is not 1D. Check the model and layer requested')\n",
        "\n",
        "    # Show the image if requested\n",
        "    if show_img:\n",
        "        plt.subplot(111)\n",
        "        dim=int(np.sqrt(m_output.size))\n",
        "        plt.imshow(m_output.reshape(dim,dim), cmap='Greys')\n",
        "        plt.show()\n",
        "\n",
        "    # Quantize\n",
        "    data_q, data_qi, scale = quantize_nbit(m_output, n_bit)\n",
        "\n",
        "    # Print this arry to verilog\n",
        "    s = \"logic signed [{}:0] test_data{} [0:{}] = '{{ {} }};\".format(n_bit-1, suffix, data_qi.size-1, ', '.join(str(int(e)) for e in data_qi))\n",
        "    print_nowrap(s)\n",
        "\n",
        "    return data_q, data_qi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "oi7TC6l81g1F",
        "outputId": "3ecb604e-2501-446f-e13d-75015f0f22e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>logic signed [7:0] test_data [0:783] = '{ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 116, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 90, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 63, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 76, 105, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 110, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 127, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 111, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 91, 127, 62, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 122, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 99, 127, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 127, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 115, 127, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 127, 60, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 127, 108, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 127, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 43, 89, 124, 127, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 127, 42, 0, 0, 0, 23, 24, 58, 72, 75, 120, 121, 117, 89, 120, 126, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 75, 126, 118, 103, 103, 103, 126, 127, 125, 120, 99, 71, 45, 14, 2, 116, 125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 88, 88, 88, 88, 88, 49, 28, 0, 0, 0, 0, 0, 51, 127, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 48, 127, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };</span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "test_data, test_data_int = test_data_to_verilog(8,x_train[2],show_img=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-zCqyoY6T6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "12fa2b87cac659b457be56b1f9fd9f19d0e5d67b8aa1ffabf71ca4ccf3ec44db"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}