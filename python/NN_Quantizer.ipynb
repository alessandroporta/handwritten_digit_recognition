{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardobove/handwritten_digit_recognition/blob/verilog_hdl/python/NN_Quantizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dHhV3Jk1g00"
      },
      "source": [
        "## Load the Dataset\n",
        "MNIST is a dataset of 70,000 handwritten images. Keras provides a function to directly download the data.\n",
        "<br>The dataset is then split into 60K training images and 10K test images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVQWFQz1g04",
        "outputId": "c436776b-6007-4b26-af92-d1242efc0513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() #load and split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH4zIJJf1g05"
      },
      "source": [
        "## Normalize the images\n",
        "The input are 28 pixels by 28 pixels images. They are represented in grayscale, which means each pixel is a single number between 0 and 255. 0 is a black pixel and 255 is a white pixel.\n",
        "<br>Before training, it is a good practice to normalize the data. In our case the min-max scaler is used to get values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mbyWnwk41g06"
      },
      "outputs": [],
      "source": [
        "# Convert datasets to float\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "\n",
        "# Normalize the pixel data\n",
        "x_train /= 255.0\n",
        "x_test  /= 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8vm9a6Z1g07"
      },
      "source": [
        "## Construct the NN model\n",
        "\n",
        "Our NN will have the following structure\n",
        "1. **AveragePooling:** This layer turns a 28x28 image to 14x14. It takes the input image and transforms each 4x4 sub-matrix by replacing it with its average value.\n",
        "2. **Flatten:** This layer converts a 14x14 matrix to a 196x1 1D vector. This conversion is needed for the next layers.\n",
        "3. **Dense layer 1:** In a dense layer (also known as a fully connected layer), the output of each neuron is calculated by a weighted sum of the inputs from all neurons in the preceding layer. This sum is passed through an activation function before being propagated to the next layer.\n",
        "4. **Dense layer 2:** The output layer has 10 neurons: one for each of the possible digit. The neuron with the largest value corresponds to the recognized digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "AAKDycnQ1g08",
        "outputId": "ef7834cb-a1e8-4c4c-bf18-cd3e14a91010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/pooling/base_pooling.py:23: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(name=name, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ average_pooling2d (\u001b[38;5;33mAveragePooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m6,304\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m330\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ average_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,304</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,634\u001b[0m (25.91 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,634</span> (25.91 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,634\u001b[0m (25.91 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,634</span> (25.91 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Importing Keras model and layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, AveragePooling2D\n",
        "\n",
        "# Construct the NN by stacking all required layers\n",
        "model_s_nn = Sequential() # Sequential: the layers will be connected to one another\n",
        "model_s_nn.add(AveragePooling2D(pool_size=(2, 2), input_shape=(28, 28, 1)))\n",
        "model_s_nn.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model_s_nn.add(Dense(32, activation=tf.nn.relu))\n",
        "model_s_nn.add(Dense(10,activation=tf.nn.softmax))\n",
        "model_s_nn.summary() # Print NN summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL54joSa1g0_"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "The model can now be trained using the training dataset. For each image x_train(i), the model will try to find the associated digit y_train(i). It will gradually adjust the values for the different weights and biases of the dense layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8NofiF01g1A",
        "outputId": "4af31c45-52c2-4653-ca6a-faf4e508e650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7563 - loss: 0.8724\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9198 - loss: 0.2777\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9337 - loss: 0.2319\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9415 - loss: 0.2053\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9474 - loss: 0.1800\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9512 - loss: 0.1676\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1522\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9609 - loss: 0.1359\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9612 - loss: 0.1294\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9651 - loss: 0.1205\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cd00b4ecfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Train the model\n",
        "model_s_nn.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']) # Train on accuracy\n",
        "model_s_nn.fit(x=x_train,y=y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6hdWkdz1g1A"
      },
      "source": [
        "## Test the model\n",
        "The accuracy of the model on the training set is approx 97%, which is pretty good.\n",
        "<br>To ensure that the model will actually perform well on new data and has not just 'learned' the training set, we need to test it on new data.\n",
        "Note that 'learning' the dataset is known as overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQV89b7b1g1B",
        "outputId": "d4fd81d9-24e8-4e77-df75-2070989a4435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9599 - loss: 0.1387\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11944250762462616, 0.9648000001907349]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Test the trained model\n",
        "model_s_nn.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaZLoZpk1g1C"
      },
      "source": [
        "As we can see, our model performs well on unseen data. We will now turn the biases and weights to Verilog vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na0-7ZVb1g1C"
      },
      "source": [
        "## Explore the weights used by the model and Quantize\n",
        "The trained model has a set of weights and biases for each neuron. The output of a neuron is being calculated as: out(n)=Sum(wn(k)*in(k))+b(n).\n",
        "<br>A 5 neurons layer which takes into input a vector of 20 elements has:\n",
        "* 5 biases, one per neuron\n",
        "* 20*5 weights, 20 per neuron\n",
        "<br><br>\n",
        "Those biases and neurons are coded as floats, which would take too much space. We will reduce these to 8 bits so that it can be stored into our board.\n",
        "<br> The verilog arrays generated are indexed as follows:  array_layer[neuron_num][weight_num]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SOYht_T-1g1D"
      },
      "outputs": [],
      "source": [
        "# Define some helper functions that will quantize an array to N bits / print the result\n",
        "\n",
        "# Returns a quantized array\n",
        "# Arguments:\n",
        "# use_scale       If non-zero it will use this instead of auto-computing the scaling factor.\n",
        "# Returns:\n",
        "# out             The quantized data\n",
        "# out_int         The quantized data, but scaled to an int value in the range +/- (2**n_bit-1)-1\n",
        "# scale           The scaling factor used between out and out_int\n",
        "\n",
        "def quantize_nbit(data, n_bit, use_scale=0, verbose=0):\n",
        "    max_bit_val = (2**(n_bit-1))-1\n",
        "    max_val     = np.max(np.abs(data))\n",
        "    if use_scale > 0:\n",
        "        scale = use_scale\n",
        "    else :\n",
        "        scale   = max_bit_val / max_val\n",
        "    if verbose:\n",
        "        print('Quantizing to +/- {}, scaling by {}'.format(max_bit_val, scale))\n",
        "\n",
        "    out_int = np.around(data * scale)\n",
        "    out = out_int /  scale\n",
        "\n",
        "    return out, out_int, scale\n",
        "\n",
        "import IPython.display as dp\n",
        "def print_nowrap(s):\n",
        "    display(dp.HTML(\"<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>\" +s+ \"</span>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "F-Xj6ezj1g1D",
        "outputId": "42b1285f-e5d2-405a-cc30-8c367b73262f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<AveragePooling2D name=average_pooling2d, built=True>, <Flatten name=flatten, built=True>, <Dense name=dense, built=True>, <Dense name=dense_1, built=True>]\n",
            "Layer 2 - Array Shape/Range: Weights = (196, 32), Biases = (32,)\n",
            "Quantizing to +/- 127, scaling by 75.98383856769239\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>localparam signed [7:0] B_ARRAY_L2 [0:31] = '{ 9, 28, 32, 3, -6, 10, 10, 17, -5, 21, -1, 25, -7, 13, 11, 2, -24, -11, 2, 5, 21, 23, 18, 13, 25, 20, 24, 0, 25, -3, -17, -17 };</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>localparam signed [7:0] W_ARRAY_L2 [0:31] [0:195] = '{<br>{ 8, 0, 9, 13, 21, 29, 32, -16, 35, 45, 31, 13, 5, -6, -7, -1, -4, 24, -7, 35, -20, -7, -15, 2, 8, 22, 29, 8, -7, -27, 2, 16, 2, -9, -6, -9, 16, 8, 8, -3, -3, 1, -22, -9, -2, -21, -42, -33, -15, -9, 9, 5, 4, -5, -2, -3, -16, -15, -32, -51, -34, -10, 3, 11, -13, -9, -19, -44, -29, -28, -15, -28, -5, -10, -13, 2, -5, -54, -36, 6, -1, -35, -47, -69, -18, -30, -11, 15, 7, 2, -5, -29, -33, 17, 20, 16, -25, -23, -44, -13, -14, 27, 34, 27, 9, 14, 2, 28, 46, 9, -3, 64, -39, -39, -5, 7, 33, 50, 24, 23, 58, 31, 26, 5, 6, 34, -21, -24, -22, 5, 8, 7, -7, 37, 34, 11, 10, 12, -6, 10, -7, 23, 28, -10, -13, 6, 10, 11, -13, -10, -25, 4, -13, -8, -2, 0, -9, 12, 2, -6, -13, -31, -48, -35, -28, 9, 8, -33, 2, -58, -11, -4, -19, -50, -42, -39, -37, 1, 0, -3, 1, 20, 5, 15, 69, 71, 24, 9, 17, 5, 1, 22, 15, 4, 17, -5 },<br>{ -3, -3, -14, -12, -2, -32, -10, -5, -12, -30, -23, -24, -10, 5, -12, -1, -33, -63, -75, -87, -78, -72, -44, -26, -12, -11, 7, -2, 0, -30, -25, -22, -11, -9, 0, 4, -8, -11, 12, 4, 9, 12, 8, -12, 11, 8, 3, 18, 14, -26, -17, 4, 10, 33, 11, 20, 52, 31, 18, 8, 18, 30, 36, 2, 7, 17, 8, 39, 65, 66, 31, -16, 13, 14, 15, 52, 44, 11, -2, -2, -5, 19, 23, 56, 50, -22, -1, -10, -12, -11, -30, -27, -37, -24, 0, 8, 33, 95, -2, -25, -54, -42, -40, -50, -3, 4, -20, -2, -19, 4, 28, 63, -28, -16, -53, -50, -43, -12, 39, 26, -6, -12, -9, -15, 34, 77, 21, -5, -7, -24, -5, 28, 35, -4, -10, -7, -33, -25, 32, 41, -3, -15, -20, -5, 9, 17, 7, -1, 1, -14, -18, -30, 26, 21, -4, -35, 1, 4, 18, 14, 17, 14, -4, 5, -1, 1, 12, -22, 6, -35, -36, -47, -19, 4, 1, 10, 5, 11, -9, -5, 41, 9, -4, -26, -51, -32, -47, -40, -37, -32, -46, -9, -41, -17, 10, -11 },<br>{ 1, -11, -16, -25, -32, -50, -40, 6, -28, -58, -44, -25, 12, 6, 2, 22, -4, 3, 0, 13, 41, 19, 13, 5, -5, -36, -18, -15, 11, 36, 35, 20, 8, 23, 20, 19, 6, 16, 10, -2, -5, 10, 32, -11, 1, -11, -10, -18, -5, 17, 6, 9, 17, -3, -3, 8, -52, -6, 4, -3, -16, -20, 5, 21, 7, 15, 5, 4, -21, 49, -51, 10, 6, -5, 1, -10, 14, 41, 12, 0, 7, -5, -2, 60, -47, -3, -9, -17, -16, -17, 27, 62, 21, -2, 2, -51, -38, 54, 12, 35, -13, -14, -30, -49, 23, 36, -10, -16, -18, -48, -29, 20, 31, 42, 8, -11, -27, -11, 38, 25, -16, -2, -19, -19, -10, -1, -28, 15, 6, 12, 8, 10, 8, 38, 14, 12, 15, 21, 16, 30, -12, 3, 4, 13, 7, 4, 7, 20, 22, 27, 25, 30, 48, -9, 13, 39, 30, -3, -3, -7, 5, 2, 18, 17, 17, 28, 21, 12, -2, -6, -5, 7, 2, 2, -5, 2, 23, 25, 21, 8, 18, 33, 12, 10, 26, 31, 19, 32, 37, 44, 59, 44, 40, 14, 23, 5 },<br>{ -3, 0, 0, 31, 27, 2, 22, -1, 49, 26, 24, 30, 6, 10, -2, 5, 23, 75, 83, 74, 51, 29, 0, -6, 6, -13, 6, 4, 9, 24, 48, 8, 20, -4, -25, -5, 17, -2, -12, -4, -23, 1, 25, 55, 51, -3, -18, -14, -12, 8, -19, -19, -31, 6, -13, -29, 38, 32, 24, 2, -14, -32, -25, -4, 13, -13, -1, -9, -79, -48, 36, 9, -26, -47, -66, -69, -5, 31, 16, -6, 12, -4, -70, -64, 12, -1, -35, -23, -7, 48, 78, -3, -13, -8, -18, 0, 36, 24, 25, 38, 13, 24, 40, 56, 31, 6, 9, 32, 30, 44, 69, 49, 32, 45, 47, 19, 33, 31, -9, 3, 32, 33, 18, 44, 27, 47, 23, 33, 36, -18, 4, -4, -7, 12, 26, -8, -19, -5, -15, 12, 29, 42, -1, -10, 7, 7, 16, 0, -3, -30, -26, -34, -32, 30, -6, 36, 0, -16, -1, 7, -3, -5, -16, -14, -22, -26, -40, 7, -5, 34, -9, -11, -23, -12, -20, -39, -36, -36, -13, -20, 6, 25, 12, -21, -58, -81, -68, -71, -86, -58, -41, -46, -81, -48, -28, -6 },<br>{ -11, 3, -18, -33, -26, -50, -47, -3, -37, -44, -53, -18, -5, -11, 8, -27, -18, -50, -73, -62, -61, -33, -15, -3, -9, 8, -24, -5, 5, 28, 34, -4, -11, 9, 21, 27, 16, 8, 3, 0, 13, -11, 4, -13, 18, -5, 28, 1, 3, 11, 14, 0, -10, -7, 12, 5, -52, 2, 11, 15, 5, -14, 5, -6, -2, 11, 6, -13, -13, 31, -63, -11, -15, -26, 2, 4, -6, 15, 21, 9, 1, 13, -25, -3, -29, -6, -11, -11, 19, 24, 26, 38, 42, 28, 0, 9, 26, -9, 6, 16, -28, -35, -4, 6, 32, 47, 47, 14, -24, -21, -43, 7, 39, 21, -37, -15, 7, -18, 11, 31, 4, -12, -16, 1, -38, -31, -34, 24, 10, 2, -4, -26, -21, -19, -13, -10, -13, -21, -61, -42, -10, -7, 1, -5, -1, -32, -25, 2, 12, -1, -8, -30, -56, -45, 18, 1, -15, -12, -5, 22, 14, 6, 5, 17, 10, -8, -17, -25, -5, -1, -26, 5, 6, 28, 18, 15, -15, 9, 0, 5, -33, -10, -7, 10, 33, 29, 19, -8, -6, -27, 7, -8, -8, -30, 5, -3 },<br>{ 5, 7, 4, -9, 10, -40, -72, 12, 26, -60, -41, -7, 3, -6, 10, 17, -14, -1, 10, -13, 14, -5, -2, 5, -22, -32, 7, 0, 3, 13, -29, -31, -34, 8, -4, -3, 0, 7, -8, -11, -2, 12, -31, -29, -28, -35, -4, 11, 16, 23, 8, -7, -2, 1, 5, -8, -53, -14, 5, -15, -8, 16, 12, 7, 0, 9, 1, 41, 35, 25, -69, -2, 29, 23, -16, -19, -13, 5, 14, 19, -5, 7, 59, 72, -42, 1, 4, -20, -4, -27, 28, 21, -13, 13, -5, -50, -44, 53, -15, -2, -25, -10, -22, -9, 16, 16, 6, -27, -42, -62, -44, 11, -15, 53, 37, -15, -15, -3, 4, 13, 20, -19, -13, -25, -9, 34, -35, -3, 29, 19, 26, 27, 1, 8, 6, 7, 2, 9, 19, 26, -18, 32, 49, 24, 11, -5, -3, -12, 14, 22, 14, 29, 45, -4, -9, 38, 7, -13, -3, -7, -10, -14, -10, -1, 21, 61, 67, -41, -10, -8, -46, -19, -23, -7, 19, 30, 35, 38, 43, 49, 60, 11, 5, 4, 29, 5, 5, 26, 28, 40, 26, 6, 22, 18, -4, 10 },<br>{ -6, -5, 6, 33, 31, 81, 71, 39, 33, 59, 62, 21, 7, 0, 8, 25, 10, 62, 55, 23, 33, 36, 40, 37, 47, 10, -25, -17, 11, -9, -16, -2, -12, 3, 5, 10, -13, 1, -42, -85, -73, -19, 0, 12, -34, -15, -1, 20, 34, 25, 23, 5, -15, -61, -110, -70, 48, 2, -8, 8, 10, 14, 8, 30, 0, -5, -22, -43, -87, -42, 68, 0, 22, 22, 20, 9, 5, 20, 4, 19, 33, 28, -12, -55, 30, 18, 44, 21, 7, -15, -44, -1, 11, 30, 21, 69, 13, -42, -8, -9, 13, 16, 11, -9, -22, -5, -14, 1, 29, 35, 18, -51, 1, -49, 0, 1, -12, -8, -19, -6, -25, 9, -8, -9, -28, -26, 48, -38, -45, -52, -26, -1, 25, -4, 5, 11, -7, -19, -8, 18, 18, -18, 5, -31, -1, 20, 43, 26, 4, 2, -12, -32, 19, 28, 21, -55, 15, -11, -4, -14, -11, 8, -2, -35, -35, -20, -1, 5, 0, -1, 26, 4, -5, -27, -19, -20, -11, -5, -5, -14, 7, -3, -12, 10, 8, 25, 83, 49, 49, 71, 62, 80, 58, 53, 14, 6 },<br>{ 6, -8, 21, 48, 39, 47, 28, 1, -6, 68, 47, 27, 12, 2, 9, -18, 35, 31, 8, 35, 13, 6, -13, 26, 53, 45, -10, 29, 6, -11, -5, -1, 32, 35, 20, 12, 1, -9, 1, 18, 76, 38, 33, 18, 28, 43, 44, 9, -7, -16, -12, -21, 4, 20, 74, 56, 25, 11, 20, 23, 28, 17, -42, -44, -2, 13, 34, 26, 65, 65, 18, 14, 20, 13, 48, 27, -42, -4, 9, -10, 12, 31, 40, 59, 54, 30, 5, 12, 36, 36, -36, -6, -18, -25, 4, -12, -25, -28, 21, -11, 9, -4, 10, 27, -3, -8, -8, 14, -11, 10, -46, -53, 5, -32, -29, -1, -5, 24, 20, 2, 16, 28, -13, -11, -14, -34, 23, 0, -37, -31, -12, 31, 26, -11, -2, -7, 0, -26, -24, -25, 13, -20, -86, -64, -28, -4, 17, 16, 11, -1, -20, -7, -19, -6, -17, -1, -19, -45, -32, -27, 1, 34, 28, -4, 3, -24, -20, 18, 0, 16, 5, -17, -16, -3, -9, 24, 26, 12, 3, 0, -26, -21, 0, 0, -32, -32, -11, -7, 10, -7, -3, 18, -2, 24, -30, 12 },<br>{ -10, 6, 1, -37, -32, -36, -33, -32, -4, -1, 6, 7, -5, 1, 8, 11, 14, 1, 32, 36, 15, 18, -5, -1, 8, -27, -29, -11, -12, 29, 12, 17, 15, 35, 13, 21, 30, 21, 4, 7, 22, -2, 18, -15, 37, 18, 18, -3, -4, -8, -1, 22, 18, 9, 43, 40, -9, -6, 17, 15, 4, 5, 9, -6, -4, 5, 15, 7, 9, 12, -36, 1, 27, 5, -16, -11, -1, -28, -26, -4, -50, -85, -82, 9, -40, -1, -7, -19, -11, 1, 7, 3, -3, 27, 7, -34, -92, -4, 11, 10, -29, -22, -17, -35, -20, 11, -12, -2, 4, -1, -11, 20, -1, 50, -36, -71, -56, -31, -2, -16, -7, 17, 10, 6, 15, 34, -16, 53, 32, -17, -36, -46, -16, 5, 25, 10, 24, 21, 12, 14, -11, 43, 46, 33, 40, 25, 27, 35, 26, 19, 19, 28, -3, 27, -19, 59, 18, 21, 27, 19, 11, 6, 15, 2, 13, 43, 1, 20, 1, 53, 5, 25, 38, 18, 40, 29, 19, 13, 1, 49, 35, -1, -7, -5, -23, -46, -62, -50, -60, -72, -76, -26, 8, -5, -6, -10 },<br>{ 5, 5, 8, 39, 44, 74, 76, 14, 46, 57, 42, 30, -10, 11, -6, 36, 40, 42, 43, 62, 31, 26, 19, 64, 57, 55, 13, 31, -11, 6, -22, 3, 21, -21, -11, 8, 18, 12, 26, 37, 35, 63, 29, -3, 22, 9, -6, -41, -35, -34, -50, -29, -12, 11, 56, 29, 59, 17, 23, 0, 3, 10, 8, -14, -18, 5, 14, 15, 35, 19, 59, 40, 13, 28, 17, 30, 55, 8, -7, 17, 2, 1, 50, 61, 50, 38, 15, 13, 28, 19, -6, -21, -10, 2, -33, -26, 36, 8, 39, 1, 6, 3, 10, -19, -26, -3, -25, -6, -11, 25, 10, -66, 19, 18, -19, -36, -28, -16, 0, -1, -10, 4, 9, 3, -7, -54, 30, 28, 19, 14, 1, 1, 9, -4, 2, -11, -21, 19, -16, -54, -8, -25, -5, 11, 42, 52, 40, 23, 9, 7, -26, 13, -47, 17, 0, -23, -20, -22, -11, 20, 7, 13, 17, 15, -25, -20, -20, 0, 5, 50, 2, -22, -27, -14, 1, 21, 18, 10, 14, 16, 11, -14, -3, 3, -50, -17, -20, -6, 5, -15, -12, 9, 5, 24, -10, 1 },<br>{ -9, -1, -8, -14, 11, -1, 11, 19, 18, -9, -4, -6, 11, 12, -10, 7, -12, -11, 14, -5, -10, -46, -15, -20, -26, -39, 22, 3, 0, 18, 21, 22, 15, 23, -25, -47, -48, -39, -36, -38, -3, 2, 34, 45, 20, 32, 27, 27, 48, 45, 6, -13, -10, 0, -34, -16, 47, 38, 33, 10, 24, 54, 66, 72, 67, 33, 4, 31, 41, 14, 39, 46, -3, -3, -8, -6, -7, 7, 27, 7, 2, 8, 28, 53, 30, -3, -41, -14, -15, -22, -26, -39, -15, -3, -23, -33, -36, 9, 22, -22, -29, -3, -5, 5, -2, -20, -32, -11, -3, -8, 34, 11, 40, 6, 8, -21, -12, -2, 1, -26, -42, -7, 24, 33, 28, 35, 30, 2, 5, -7, -4, 5, -24, -25, 1, 7, 28, 50, 11, 8, 11, -9, 19, -14, -29, -15, -15, -13, 6, 17, 13, 16, 3, 15, 6, 37, -8, 3, -40, -17, 6, 11, 2, -14, -13, 3, 17, -5, 11, 42, 59, 38, 46, 51, 54, 40, 37, 12, 15, 19, 27, -14, 10, -14, 13, 38, 78, 108, 108, 100, 99, 85, 48, 56, 2, 12 },<br>{ 3, 10, 13, 19, 41, 24, 16, -17, 28, 57, 31, 32, 7, -8, 2, 0, 19, 24, 43, 39, 23, 3, -12, 7, 10, 26, 31, 23, -3, -11, -4, 2, 1, 16, 4, -1, -3, 11, -14, 7, 19, 4, -14, -17, -1, -22, -22, -3, -12, -15, -8, -16, -33, -48, 2, 18, -22, -22, -30, -33, -9, 2, -17, -23, -45, -29, -4, -34, -18, -18, -36, -23, -2, -8, -19, -21, -8, -23, -21, -20, 2, -16, -3, -22, -23, -40, -30, -29, -22, 5, 33, 6, -4, -10, -23, -5, -5, -4, -6, 18, 5, 13, 6, 24, 32, 14, 28, -2, -5, 16, 32, 48, -35, 10, 23, 34, 6, 2, 9, 29, 26, 29, 18, 9, 34, 23, -27, 8, 23, 43, 25, 17, 16, 28, 46, 23, 12, 6, 26, 39, -21, 40, 5, -2, 18, 3, 2, 17, 17, 25, 18, 8, 17, 21, 7, 22, -4, -40, 1, 4, -23, -46, -47, -27, 2, -3, -14, 0, -7, -38, -10, -20, -24, -40, -47, -42, -49, -50, 6, 2, 16, 30, -10, 7, -5, -10, -4, -18, 5, 17, 26, 9, 21, 8, -5, -9 },<br>{ -11, 2, 9, 19, 25, 64, 42, -9, 43, 43, 49, 41, -2, -7, -8, 15, 11, -10, 23, 1, -2, -14, -6, 20, 37, 23, 34, 10, 10, -36, -48, -32, 7, 29, 31, 23, 10, 15, 9, 2, 37, 50, -21, 14, -24, -1, 15, 47, 46, 43, 23, 18, 5, -18, 12, -9, -3, -16, -16, 18, 31, 26, 3, -16, -19, -2, -14, -33, 30, 20, -4, -1, 26, 43, 23, 4, -18, -21, -5, 10, -7, -2, 53, 68, 10, 37, 35, 11, -9, -16, -20, 25, 20, 14, 11, 14, -4, 11, 8, 5, -30, -27, -7, -3, -19, 4, 1, 13, 21, -4, -42, -72, -30, -15, -9, -9, 5, 4, 2, -25, -6, 4, -2, -13, -19, -47, 17, -51, -2, 18, 2, 14, -4, -33, -10, 10, -5, -12, -17, -35, -6, -41, 1, 0, 5, -9, -4, 0, 9, -7, -20, 3, 9, -23, -4, -13, -21, -17, -2, 1, 14, 18, 5, -22, 12, 31, 17, -16, 4, 3, -5, -10, -8, 1, 13, 19, 19, 41, 35, 7, -1, -24, -1, 1, 19, 21, 68, 72, 86, 100, 78, 65, 88, 34, 13, -6 },<br>{ -1, -10, 10, 26, 27, 60, 59, 15, 42, 68, 66, 40, -12, 9, 11, 25, 8, 8, 28, 20, 23, 13, 24, 17, 31, 50, 7, 13, -8, -25, -67, -29, -8, -3, 8, 14, 33, 22, 35, 36, 29, 45, -33, -49, -31, -13, -12, 1, -1, -7, -5, 11, 4, 18, 57, 24, 10, -25, -52, -13, -3, 9, 7, -10, -2, -1, 0, -5, 52, 0, 10, -59, -50, 2, 24, 5, 5, -17, -22, -18, -5, 3, 66, 21, -5, -26, -11, 28, 20, 9, 27, -14, -43, -9, 5, 20, 11, -40, -16, -14, 6, 31, 35, 27, 22, -4, -35, -26, -11, 18, 32, -61, -35, -2, 21, 6, 22, 39, 4, -9, -17, -14, -8, 15, 4, -65, -27, 1, 11, 29, 25, 22, -7, 11, 2, 9, 8, 19, 17, -70, 11, 12, 5, 17, 18, 22, 36, 33, 31, 17, 16, 31, 2, 7, 2, -6, 8, 9, 17, 18, 27, 30, 27, 7, 8, 8, -20, 22, -2, -4, -43, -34, -15, -6, -13, -7, -16, 2, -27, 1, -10, -16, 3, 3, -34, -87, -97, -29, -15, -58, -46, -34, -44, -43, -14, -7 },<br>{ -3, 2, 11, 10, 20, 14, 5, 12, 28, 8, 13, -2, 6, -9, -4, -19, -20, -11, 24, 31, 25, 33, 7, 33, 2, -22, 25, 15, -2, -7, -23, -14, 5, 14, 23, 15, 9, 10, 6, 9, -25, -8, 26, 18, 10, 6, -1, 15, 25, 31, 16, 27, 13, 1, 16, -13, -12, 27, 10, -9, -13, -8, 19, 28, 1, -4, -2, 14, 15, 3, -18, 18, -19, -28, -4, -2, -10, -69, -36, -20, -22, -2, 3, 5, -29, 23, -22, -7, 8, -7, -31, -60, -26, -25, -17, -12, -29, -20, -2, 5, -13, 7, 3, -13, -14, -17, -30, -41, 16, 4, 14, 44, -23, 10, -14, -24, -13, -9, -21, -27, -26, 2, 11, -9, -4, 43, -8, 11, -29, -14, -16, -36, -49, -26, 1, 5, 2, -6, -31, 28, -16, 45, -14, -4, 16, 17, 43, 61, 40, 13, 12, 18, 19, 57, -5, 25, 16, 13, 36, 52, 48, 47, 48, 36, 18, 19, 6, 25, 6, 30, 12, 28, 24, 7, 14, 17, 20, 33, -5, 16, 57, 7, -11, 8, -2, -40, -28, -12, -29, -55, -59, -28, -29, -16, -5, -4 },<br>{ -11, 6, 2, 7, 21, 35, 12, 1, 41, 64, 39, 12, 10, -8, -8, 4, 10, -23, -14, -13, 13, 34, 20, 13, 7, 31, -23, 3, -11, -24, -27, -30, -22, -22, -10, 5, 16, 10, 7, -25, -24, 23, -42, -16, -53, -51, -40, -35, -9, 19, 13, 13, 12, -2, -3, -10, -30, -6, -36, -62, -55, 4, 29, 11, -7, -11, -11, 6, 3, -10, -25, -14, -43, -33, -4, 2, 16, 14, -10, -7, 7, -22, -57, -23, 10, -28, -21, 13, 11, -16, 19, 17, 31, 11, 8, -13, -48, -52, 3, -28, -16, -16, -12, 8, 23, 15, 1, -17, -40, -38, -30, 11, -27, -3, -34, -15, 46, 56, 0, -13, -16, -48, -23, -19, -21, 68, -42, -49, -54, -18, 43, 30, -30, -33, -17, -52, 0, 27, -11, 25, -29, -45, -14, -3, 12, 7, 2, -5, -18, 3, 25, 30, 3, -13, -4, -4, -7, -14, -6, 7, 20, 17, 17, 6, -13, 8, -13, -45, 6, -43, -48, -32, -13, -8, -3, 13, 30, -9, -17, -18, -14, -26, 9, 15, 21, 38, 51, 34, 32, 49, 47, 50, 10, -26, 4, 9 },<br>{ -6, 1, -7, -13, -48, -17, 3, -10, -23, -57, -57, -34, 6, 4, 4, 4, -29, -33, -10, 1, 14, -10, -19, -35, -26, -55, -40, -17, 13, -5, -17, -11, -1, 30, 32, 40, 28, 18, -13, -21, -46, -46, 2, -22, -23, -36, -23, -10, 21, 28, 18, 10, 7, 10, -62, -83, 2, 2, -21, -20, -6, -20, -42, 5, 44, 29, 18, -5, -48, -95, -14, 0, -4, 21, 8, -25, -21, -12, 33, 32, 26, 24, -25, -78, -49, -3, 3, 11, 8, -4, -4, -13, -27, 15, 14, 23, -7, -45, -2, -24, 41, 14, 0, -32, -20, -43, -24, 29, 23, 14, -9, 18, 37, -10, 31, 5, -2, -12, -12, -52, 11, 20, -6, -12, -31, 24, 9, 12, 5, 31, 17, 41, 2, 1, 42, 15, -8, -20, -23, 28, 21, 14, 32, 16, -1, -8, 12, 19, 26, 2, -24, -16, -31, -10, 9, 37, -27, 2, -9, 6, 11, 19, 14, 10, 13, 23, 24, -44, 7, 28, -9, -30, -32, -9, -11, -14, -8, -4, 27, 39, 48, 7, -7, -12, 33, 29, -3, -5, -4, -6, -26, -3, 47, 9, 29, -9 },<br>{ -5, -7, 11, 42, 30, 73, 51, 19, 11, 71, 67, 27, -12, -9, 5, 13, 15, 23, 19, 16, 28, 34, 24, 38, 39, 62, 23, 9, 9, -4, -26, -4, 25, 9, 22, 41, 41, 38, 31, 18, 34, 45, -10, -64, 1, -12, -9, 22, 14, -3, 20, 15, -1, 24, 32, -4, -17, -1, -17, -8, 12, 1, 12, 2, 4, -2, 3, 7, 43, 20, -13, -20, -9, -10, 4, 5, 29, 0, -26, 0, 5, 22, 35, 1, -42, -8, 4, 14, 8, 3, 28, 1, -20, -4, 19, 44, 54, -36, 8, -36, 12, 13, 33, 14, 20, 18, -8, -3, 10, 0, -9, -46, 13, -14, -2, 29, 38, 35, 1, -7, -11, -1, 17, -6, -13, -73, -14, 2, 8, 19, 16, 1, -32, -17, 0, 14, 23, 23, -19, -89, 16, -30, -5, 33, 31, 16, 30, 24, 22, 25, 15, 12, -42, 10, -13, 11, -1, 6, 22, 27, 31, 21, 25, -5, 13, -1, -28, 13, -6, 22, -55, -28, -11, 25, 36, 37, 2, 2, -8, -13, -42, -15, 3, -11, -18, -46, -45, -42, -45, -67, -28, -37, -36, -31, -19, -2 },<br>{ 10, 2, -6, 21, 5, 24, 17, 31, 12, 14, 19, 1, 8, 0, -12, 2, -18, 5, 10, 14, 14, 27, 12, 7, 8, 3, -1, 11, -5, -26, 10, 14, 10, -6, -12, -1, 10, -1, 25, 6, -20, -32, 11, 14, 18, 12, -10, 5, -9, -2, 7, 12, 14, -7, -17, -9, 20, -6, 15, 19, 4, -7, 3, 8, 17, 17, -2, 3, 8, -3, 32, 8, 4, 9, 13, -2, -14, -23, -19, -10, 2, -2, -2, -18, 20, 13, 21, 9, 17, 12, -35, -28, -1, -9, 3, -8, -8, 7, 5, 17, 29, 19, 10, -5, -53, -9, -5, -12, -22, -29, -16, -13, 5, 13, 0, -11, -19, -66, -35, 35, 21, 4, -4, -39, -39, 2, 18, -5, -23, -41, -41, -53, 8, 45, 33, 14, -6, -16, -16, 5, -7, 16, -25, -23, -16, -4, 13, 31, 31, 29, 11, -1, 2, -12, 8, -9, 10, -21, -21, -26, -6, 13, 12, 30, 5, 10, 5, 0, 8, 3, -8, 3, -4, -15, 7, 5, 14, 6, 12, -21, -12, -12, -12, 2, -8, -16, 0, -1, 4, -1, 5, 17, 9, -11, -9, 8 },<br>{ 9, 3, 13, 30, 34, 72, 67, 38, 38, 49, 54, 22, 3, 11, 7, 3, 42, 96, 62, 60, 36, 22, -21, -11, -7, 1, 6, 25, 4, 48, 55, 33, 32, 15, 12, -6, -36, -46, -32, -39, -2, 43, 40, 67, 32, 19, 4, -3, 11, 0, 14, -24, -34, -26, -4, 19, 58, 68, 41, 8, 1, -21, 8, 44, 2, -28, -17, -24, -10, 40, 39, 42, 37, -2, -22, -11, 8, 11, 0, 12, 4, -23, -38, -19, 45, 6, 11, -22, -5, -21, -9, 36, 42, 17, 10, 11, -8, -33, 1, -2, -31, -22, -47, -20, 15, 49, 18, 0, -2, -12, 11, 12, 47, 41, -34, -57, -34, 29, 68, 38, 10, -11, 13, -15, 7, 17, 22, 15, -26, -43, -30, 18, 43, 18, -1, 9, 14, 7, -23, 56, 0, 12, -26, -21, -19, 5, 10, 8, -7, -15, -15, -23, -1, 5, 21, 10, -5, 16, 8, 7, 0, 13, -7, -4, -9, -29, -15, -19, -6, 11, 26, 42, 11, 8, -6, 25, -6, -17, -22, -38, -27, 19, 11, -9, -33, 25, 19, -6, -25, -23, 1, 27, -22, -40, -11, 0 },<br>{ 3, -3, 10, -6, -4, 24, 32, -13, -45, 13, 5, 20, 2, -5, -2, -1, -6, -29, -72, -70, -56, -33, -29, -14, 3, -18, -41, -9, -7, -28, 27, -12, -71, -45, -2, -3, -20, -22, -22, -3, -2, -12, -46, -3, -14, -40, -55, -13, -13, -2, -1, 2, -1, 10, 20, 8, -56, -25, -34, -42, -42, -24, 31, 26, 4, 18, 20, 17, 55, -9, -10, -58, -69, -62, -58, -18, 54, 23, 6, -7, 1, 25, 58, 28, -3, -48, -87, -43, -25, 29, 71, 50, 9, -13, -37, -57, -68, -12, 25, -27, -41, -39, -32, 9, 33, 26, 10, -16, -7, -74, -50, -54, -34, -20, 0, -23, -41, -14, 7, 22, 37, 0, -39, -54, -47, -27, -4, -77, 0, 9, 3, -1, 19, 34, 32, -40, -79, -48, -15, 36, 30, -41, -5, -12, 0, 3, 1, 13, 1, -57, -114, -36, 6, 20, 2, -27, -20, 4, 15, 25, 21, 0, -20, -64, -86, -22, 3, 10, 0, -6, 1, -19, 0, -1, 0, 0, -13, -21, -4, 30, 33, -14, -2, 10, 21, -7, -32, -5, -13, 26, 46, 4, -20, -41, -13, 2 },<br>{ -11, 4, 4, 11, -1, -1, 9, 31, 46, 6, -10, 16, -11, 11, -6, 38, -13, 23, 4, -23, 4, 14, -2, -13, -6, 4, 10, 5, 6, 12, 35, -1, -5, -2, 13, 1, -8, -6, -7, 1, -14, -34, 32, 17, 3, 15, 6, 25, 11, 6, 18, 17, 15, 2, -37, -8, 30, 4, 4, 5, 30, 15, -6, 28, 31, 10, -7, -2, -57, 4, -9, -46, -26, 10, 4, -58, -106, 17, 6, -28, -33, -28, -15, -5, 16, -38, -59, -103, -113, -116, -79, 8, -22, -19, -31, -35, -8, 26, -16, -26, -72, -60, -24, 17, 35, 27, 3, 16, 12, 17, -4, 36, -26, -62, -34, 35, 17, 33, 44, 10, -21, -13, -8, -26, 33, 30, -30, -3, 2, 24, 24, 13, 46, 2, -16, -10, -4, -16, 50, 73, -16, 18, 12, -2, 10, 11, 24, 17, 0, -1, 7, 12, 34, 27, 2, -6, 54, 20, -4, 2, 2, -4, -6, -12, 2, -25, -31, 19, 2, -30, 8, -7, -16, -20, 3, -13, -32, -66, -58, -30, 15, 35, -1, -11, -24, -11, 15, -2, -22, 4, -37, -37, -30, -15, 17, 11 },<br>{ -2, 4, -21, -34, -31, -44, -23, -26, -13, -38, -24, -27, 2, 7, -7, 2, -1, -82, -89, -98, -127, -96, -68, -7, -16, -27, -28, -19, 19, 25, 14, -3, -24, -23, -11, -28, 6, -7, -12, 8, 13, -1, 13, -1, 12, 6, 14, 12, 17, 6, 6, -9, -2, -1, 12, 21, -51, 18, -12, 16, 13, 8, 1, 2, 15, -2, -5, 10, 13, 1, -37, 21, -3, 11, 3, -21, -23, -13, -5, -11, 11, 8, 8, 21, -41, -27, 25, 27, 29, 26, 14, 24, 2, 5, 15, 19, -11, -1, -28, -1, -3, 22, 67, 44, 45, 38, 31, 0, 0, 16, -22, -19, 4, -34, -53, -17, 4, 35, 42, 42, 0, -20, -13, -3, -44, -33, -34, -25, -63, -89, -113, -83, -60, -20, -8, -20, -5, -21, -43, -16, 18, 16, -6, -26, -33, -26, -29, 2, 0, 0, 7, 0, -2, -28, 7, 70, 31, 15, 32, 18, -18, -8, -9, -10, -5, 13, 19, 31, -1, 26, 58, 35, 27, 26, -4, -8, -2, -12, 7, 17, -35, -8, 4, 12, 50, 37, 23, 34, 44, 38, 44, 23, 17, -4, 22, 0 },<br>{ -8, 11, 21, 36, 35, 65, 44, -14, 13, 73, 70, 37, -9, -6, 10, -7, -2, 8, 12, 42, -8, -1, -7, -13, 7, 25, 23, -5, -6, -10, -6, -1, 18, 18, 12, -3, 3, -5, 0, 8, 15, 12, -25, -25, 13, 7, 10, -13, -19, -9, -24, -52, -31, 10, 6, 7, -31, 2, 16, -6, 3, -16, -22, -8, -1, -10, -11, -24, -11, -22, -18, -4, 8, 9, 1, -16, -49, -58, 9, -4, -14, -12, -7, -64, -15, -11, 2, 22, 21, 28, -24, -51, -7, 17, 8, 32, 34, -57, -12, 4, 14, 23, 16, 13, 22, 24, 14, 2, 6, -3, 23, 52, -10, 6, 13, 8, -21, 6, 33, 37, 20, -6, -11, -8, 6, 38, -34, -6, -18, -6, -45, 32, 58, 21, 17, -10, -4, -1, 22, 6, -12, 14, -24, -22, -3, 35, 27, 29, 21, 16, 8, 13, 2, 19, 0, 15, -20, -52, -10, -7, 11, 12, 8, 31, 32, 39, 0, -27, -5, -44, -51, -53, -61, -32, -21, -10, -2, 7, 8, 30, 18, 12, -11, 12, 10, 43, 7, 14, 5, -32, -47, -40, -21, 11, -27, -3 },<br>{ -3, 6, -10, -24, -56, -65, -61, -12, -30, -70, -61, -43, -7, 8, -3, -22, -18, -58, -94, -48, -83, -47, -40, -9, -8, 20, 11, 16, -13, -14, -27, -21, -14, -20, -5, -1, -14, 2, 9, 9, 31, -18, -17, 19, -1, -2, 10, 17, 26, 21, 27, 5, 15, 10, 32, -5, 18, 5, 2, 4, 21, 14, 19, -3, -16, 2, 15, 25, 46, -6, 25, -15, 14, 21, 53, 30, -5, -29, -3, 15, 16, 1, -20, -20, 42, 6, 19, 35, 21, 35, -12, -15, 16, 30, 46, 30, -17, 15, -29, 3, 20, 43, 11, 21, -11, -7, 34, 48, 26, 10, -19, 37, -44, -32, -4, 41, 19, -10, -11, 28, 48, 26, 20, -21, -22, 15, -13, -26, 8, 17, -19, 7, -3, 21, -1, 4, -19, -29, -13, 14, 13, -16, -7, -36, -19, -8, -2, -20, -22, -45, -52, -28, -6, -14, -12, -34, -27, 15, 21, 26, 11, -5, -19, -38, -28, 3, 13, -33, -12, -5, 12, 14, -15, 9, 1, 18, -2, 11, 47, 32, 1, -29, 4, 23, 10, 40, 34, 12, 17, 9, -13, 3, 23, 22, 5, 5 },<br>{ 4, -3, -10, 30, -8, -11, -8, -8, -11, 22, 33, 20, -2, -11, 10, -24, 11, 35, 23, 33, 25, 19, -9, -13, -5, 13, 15, -13, 8, 3, 46, 5, 23, 4, 8, 15, 20, 16, 10, -2, -14, 10, 9, 6, 15, 9, 5, 18, 19, 7, -14, -7, 1, 9, 5, 17, -38, 4, 35, 16, -5, -1, -8, 2, -3, -5, -12, -13, -57, 2, -37, -11, 1, 3, -7, -16, -8, 4, 3, 8, 9, 3, -57, -44, -26, -29, -16, -36, -6, -13, 14, 36, 21, -14, 1, 19, 38, -14, -16, 0, -28, -22, -18, 16, 20, 50, 45, -4, -6, -12, -16, 65, 8, 1, -12, 5, 4, 24, 30, 44, 23, -10, -12, -17, -5, 57, -41, -9, -23, -5, -8, -5, 19, 22, 26, -7, 0, -4, 12, 52, -13, 25, -11, 21, 7, 5, 15, 33, 32, 20, -1, 10, 15, -12, 7, 25, 6, 18, 17, 11, 2, 8, -2, 11, 23, 4, 5, -25, -9, -54, -56, -13, -4, -20, -5, -17, 16, 3, 8, 3, -6, 26, 7, 17, 23, 22, 8, -18, -21, -25, -9, -5, -13, -18, 22, 4 },<br>{ 2, -4, 4, -6, -19, 5, -8, 6, -2, -34, -43, -25, 9, 4, -11, 1, 8, -25, -17, -43, -59, -25, -33, -43, -32, -28, -29, -2, -7, -5, -19, -24, 9, -34, -46, -33, -62, -63, -39, -40, -22, -28, -12, 48, -7, -4, -8, -5, -3, -2, 0, -9, -3, 15, -27, -13, 32, 12, 11, -3, -17, -24, 15, 30, 27, 16, -3, -1, 16, -29, 61, 13, -26, -24, -17, 14, 58, 57, 40, 22, -6, -14, 8, -8, 34, 31, -30, 3, 29, 37, 8, 11, 28, 3, -49, -60, -3, 4, 30, 12, -4, 23, 25, 29, -10, 5, 52, 13, -12, 11, -1, -56, -33, -50, 19, 5, 14, 18, -18, 19, 35, -18, -8, -22, -36, -20, 27, -41, -24, -29, -26, -6, -2, 19, -23, -34, -14, -45, -29, 6, 6, -51, -22, -47, -36, -21, 1, -31, -37, -44, -48, -45, 24, -3, 2, -86, -27, 1, 1, -8, -24, -24, -22, -21, -15, -25, 16, -10, -5, -10, 39, -6, -14, -13, -24, -16, -13, -5, -7, 11, 36, 11, -10, 21, 13, 16, 25, 11, 21, 48, 30, 42, 50, 46, 13, 8 },<br>{ 12, 3, 6, -22, 25, 40, 54, -1, -32, 40, 35, -12, -12, 3, 8, -7, -5, 10, -9, 35, 8, 3, -22, -8, -10, 8, 20, 11, 10, -2, 5, 8, 0, -1, -18, -20, -19, 41, 36, 18, -19, -28, -2, 0, 21, 13, -1, 2, -9, -57, -30, 27, 20, 11, -25, -19, 7, 7, 15, -12, 15, 15, 11, -67, 25, 42, 6, -1, -45, -50, 31, -18, -23, -9, 7, 3, -39, -74, 53, 38, -3, -21, -65, -63, 4, -18, -6, 7, 8, 3, -29, -25, 36, 8, -10, 9, -8, -50, 14, -15, 8, 5, 12, 4, -19, -12, 19, -3, -2, 1, -23, -32, 21, -19, -14, -5, 18, 3, -12, 5, -10, 12, 25, -1, -14, -35, -17, 27, -8, -1, 23, 16, 7, -9, 1, 9, 14, -7, -19, -31, 15, 16, 18, -5, 10, 35, 7, -7, 2, -2, 1, -2, -19, 15, -1, -21, 19, 35, 20, 10, 3, 5, -5, 6, 17, 2, -16, 19, 10, 32, 22, 6, -16, -13, -3, -5, -3, -39, -35, -34, -34, -4, -12, -26, -6, -22, -58, -45, -44, -44, -47, -70, -69, -24, -7, 11 },<br>{ -12, -11, -11, -16, -22, -33, -28, -5, -37, -29, -28, -29, 7, -11, 12, -10, 16, -9, -25, -23, 10, 21, 26, 3, 34, -11, -32, -6, 15, 23, 39, 44, 28, 19, 8, 15, 9, 14, -18, 11, 2, -29, 32, 48, 20, 25, 11, -6, -15, -13, -2, -5, -13, -9, 22, 25, 16, 8, 15, 4, 14, 3, 21, 10, -11, -12, -2, 11, 12, -4, 40, 33, -2, -3, 1, 34, 26, 0, 12, -9, 2, -15, -49, 10, 62, 3, -6, 7, 27, 34, -22, -15, 18, 15, 1, -23, -61, -12, 17, 41, 23, 19, 1, -35, -56, -2, 18, 19, 31, 22, -40, -14, 34, 17, 3, 7, -7, -31, -49, 25, 25, 29, 22, 11, -11, -33, 24, 45, 19, 5, 6, -40, -55, 40, 33, 12, 16, -3, -31, -21, 5, 10, -31, -12, 0, -35, -7, 32, 34, 28, 8, 5, -8, -23, 20, -15, -16, -38, -30, -11, -7, 7, 15, 16, -5, -37, -5, 33, -12, 33, 44, 28, 9, 3, 4, 12, 19, -4, -1, -10, -12, -11, -3, -10, -21, 10, 26, 16, -4, 7, 21, 16, 3, 27, 1, 0 },<br>{ -2, -1, -2, 1, -7, -17, -30, -24, 47, -33, -25, -3, 9, -3, -6, -12, -10, 6, 27, 42, 33, 19, 20, -3, 5, -18, -1, -7, -6, 22, 3, 15, 31, 32, 42, 34, 31, 4, -4, -7, -29, 19, 35, 64, 35, 29, 42, 28, 36, 21, 28, 23, -4, -4, -42, -6, 50, 53, 6, 6, 10, 6, 19, 27, 23, 51, 15, 24, -84, -21, 45, 14, -38, -35, -62, -44, -33, -29, 16, 43, 38, -35, -59, -27, 14, -36, -73, -50, -47, -21, -27, -29, 14, 9, -30, -73, -26, 19, 22, -4, -6, -1, -13, -9, 0, 1, 11, 11, -35, -41, -11, 56, 29, 7, 13, -19, -12, -22, -19, -12, 21, -22, -13, 0, 17, 57, 21, 0, 19, 7, 9, 6, -9, 1, 16, -1, 2, 1, 11, 51, 35, 26, 3, 11, 17, 8, 6, 1, 7, -12, -1, -8, -13, 16, -6, 15, 5, 8, -5, 4, 0, -3, -6, -6, -11, 26, -3, -28, -2, 10, 64, 38, 28, 26, 37, 16, 11, 1, 2, -18, 25, 12, -10, -2, -33, 21, 36, 32, 27, 42, 58, 62, 29, -9, 16, 2 },<br>{ -10, 1, 5, 16, 30, 33, 82, 54, 86, 75, 27, 27, 8, 0, 12, 36, 26, 20, 5, 4, 9, 21, 38, 50, 69, 50, 32, 13, 14, 33, -14, -40, -3, 10, -5, -4, 8, 33, 32, 29, 41, 24, 16, -19, -15, -39, -12, -3, 12, 36, 47, 38, 41, 7, 55, 32, 5, -9, -43, -21, 4, 2, -6, -7, -46, -59, -33, -20, 1, 35, -42, -5, -32, -14, 15, 11, 4, -19, -46, -50, -99, -102, -42, 37, -29, 3, -38, -23, -3, 12, 17, 17, 39, 14, -35, -94, -94, -20, 30, -22, -44, -49, -19, 2, 14, -14, 23, 19, 25, -3, -64, -30, 24, 23, -4, -30, -46, 5, -8, -17, -7, 38, 29, -20, -76, -40, -6, 62, 22, -2, -22, -35, -47, -13, 21, 42, 14, -36, -37, -41, 7, 0, 21, 2, 17, 13, 25, 7, 15, 23, 10, -20, -27, -9, 0, -6, 0, -19, -14, 14, 14, 17, 4, 12, -4, -33, -18, 8, -7, 47, 55, 62, 37, 35, 20, 11, 5, -27, -26, 2, -21, -10, 3, 12, 42, 33, 49, 35, 15, 7, 25, 13, 11, 13, 12, 12 },<br>{ 9, -12, -7, 7, 5, 11, -5, -15, 24, -6, 13, 6, -9, 8, 3, 14, 13, 19, 39, 54, 58, 32, 7, -13, -19, -19, 2, 6, -2, 3, 15, 26, 28, 41, 37, 27, 25, 9, -1, -21, -19, 17, 46, 56, 18, -9, -5, -14, -4, 31, 43, 56, 36, 11, -23, -45, 50, 18, -2, 5, -15, -28, -12, 17, 34, 42, 37, 36, -5, -7, 47, 35, -26, -14, -19, -6, -3, -5, -10, 24, 47, 72, 7, -4, 13, -14, -12, -26, -2, -13, -13, -3, -9, -12, 0, 19, 34, -12, 13, -2, 7, -13, -15, -9, 4, 11, -2, -2, -5, -16, -18, 11, 41, 9, 23, 9, 2, 13, -29, -21, 17, -14, -37, -14, -13, -9, 35, -16, 4, 10, 23, 3, -51, -5, 19, -28, -24, -17, -28, 3, 23, -13, -1, 38, 19, 7, 14, 11, -14, -26, -19, -18, -31, 14, -16, 28, -10, 30, 20, 32, 37, 8, 6, -6, -45, -22, -4, -24, 2, 22, 30, 32, 14, 18, 29, 10, -2, -24, -28, -54, -37, -14, 1, -21, -13, 25, 51, 45, 40, 36, 37, 58, 25, 11, -4, 7 }<br>};<br></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 3 - Array Shape/Range: Weights = (32, 10), Biases = (10,)\n",
            "Quantizing to +/- 127, scaling by 52.1668464397967\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>localparam signed [7:0] B_ARRAY_L3 [0:9] = '{ -14, 19, 0, -7, 7, 13, -14, 6, -5, -10 };</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>localparam signed [7:0] W_ARRAY_L3 [0:9] [0:31] = '{<br>{ 7, -20, -51, 11, -36, -19, 25, -11, -39, 4, 33, -40, -4, 21, 47, 4, 32, 10, -48, 11, 18, -53, -121, 7, 4, -42, 18, 24, -37, 23, -10, 20 },<br>{ -91, 19, 33, -15, 9, 8, 44, -113, -13, -27, -64, -7, -76, 6, 29, 0, -55, -4, 43, 12, -10, 42, 4, -20, -89, 16, -15, 69, 41, -30, -18, -40 },<br>{ 18, 61, 0, 23, -22, 29, -20, -35, 31, -5, 2, 46, -14, -6, 45, 51, 6, -25, -6, 10, -123, 27, -51, 24, 3, 16, -5, -27, -30, 27, -87, 4 },<br>{ -28, -35, 24, 37, 12, 17, -11, -25, 23, 24, 34, -18, -10, -18, -2, -73, 12, -10, -44, 25, -33, -16, 34, -48, -36, 0, -49, -9, 6, 33, 16, 28 },<br>{ 8, 2, -17, -15, 31, -21, -66, 8, -31, -35, -75, -10, -33, 3, -127, -17, 8, -4, -3, -12, 26, -30, 25, 18, 31, 0, -2, 53, 37, -18, -63, -93 },<br>{ -29, 17, -4, -22, -39, 28, -18, 33, 50, 21, 19, 26, 22, 26, 25, -34, -36, -3, -23, -90, 52, 13, 16, -9, -12, -39, 28, -64, -13, 0, 57, -29 },<br>{ 7, -35, -25, 7, -34, -45, 24, 12, -71, 16, -60, 35, -1, 24, 25, 49, -35, 5, -1, 23, 11, -11, -98, 44, -18, 11, -47, 11, -16, -86, 32, -25 },<br>{ -42, 24, -22, 3, -17, -36, 23, 16, -45, 16, 10, -35, 7, -43, -19, -49, -8, -40, 22, 27, -29, 65, -11, -81, 28, -18, 40, 8, 49, 43, -73, 15 },<br>{ -24, 33, 2, -30, 34, 14, -21, 17, -4, -1, 0, -40, 27, 17, -84, 30, -43, 32, 35, 10, -13, -60, -31, 20, -3, 0, -71, -43, -57, 5, -14, 25 },<br>{ 32, -44, -1, -115, 18, 15, 25, -7, -44, -79, -1, -41, 30, -31, -27, 32, 55, -29, -34, -14, 20, -90, 28, -19, 25, -1, 35, -36, 5, -3, 11, 1 }<br>};<br></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Quantize and Export the weights\n",
        "n_bit               = 8            # Number of bits to quantize to\n",
        "quantized_wb        = [0,0,0,0]\n",
        "quantized_wb_scale  = [0,0,0,0]\n",
        "\n",
        "print(model_s_nn.layers)\n",
        "\n",
        "for l,layer in enumerate(model_s_nn.layers):\n",
        "    if len(layer.weights)>0:\n",
        "        w,b = layer.weights\n",
        "        w,b = w.numpy(), b.numpy()\n",
        "        print('Layer {} - Array Shape/Range: Weights = {}, Biases = {}'.format(l, w.shape, b.shape))\n",
        "        w_q, w_qi, scale = quantize_nbit(w, n_bit, verbose=1)\n",
        "        b_q, b_qi, _     = quantize_nbit(b, n_bit, use_scale=scale)\n",
        "\n",
        "        # Print the scaled values\n",
        "        num_w, num_neuron = w.shape\n",
        "\n",
        "        # Print the Biases as an SV parameter array\n",
        "        s = \"localparam signed [{}:0] B_ARRAY_L{} [0:{}] = '{{ {} }};\".format(n_bit-1, l, num_neuron-1, ', '.join(str(int(e)) for e in b_qi))\n",
        "        print_nowrap(s)\n",
        "\n",
        "        # Print Weights as an SV parameter array\n",
        "        s = \"localparam signed [{}:0] W_ARRAY_L{} [0:{}] [0:{}] = '{{<br>\".format(n_bit-1, l, num_neuron-1, len(w_qi[:,0])-1)\n",
        "        for n in range(num_neuron) :\n",
        "            # Note: you can change\n",
        "            #s += \"Layer {}, Neuron {}, Bias = {}, Weights = {}<br>\".format(l, n, int(b_qi[n]), ', '.join(str(int(e)) for e in w_qi[:,n]))\n",
        "            #s += \"bias_l{}[{}] = {}; weight_l{}[{}] = {{ {} }}<br>\".format(l, n, int(b_qi[n]), l, n, ', '.join(str(int(e)) for e in w_qi[:,n]))\n",
        "            s += \"{{ {} }},<br>\".format(', '.join(str(int(e)) for e in w_qi[:,n]))\n",
        "        s = s[0:-5] # remove last comma\n",
        "        s += \"<br>};<br>\"\n",
        "        print_nowrap(s)\n",
        "\n",
        "\n",
        "        # Save the quantized weights/bias for use later\n",
        "        quantized_wb[l]       = (w_qi, b_qi)\n",
        "        quantized_wb_scale[l] = (w_q,  b_q)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bTRb7IW1g1E"
      },
      "source": [
        "The weights and biases are available above and can be added to the Verilog code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkPJ7iiW1g1E"
      },
      "source": [
        "## Creation of test vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Jr3xDB1g1F"
      },
      "source": [
        "This function transform images in test vectors to be used in the Verilog code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zE3XXP6j1g1F"
      },
      "outputs": [],
      "source": [
        "# Write data out in the form\n",
        "# logic signed [7:0] data [0:N-1] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 4, 0, 0, 0, 0, 53, 0, 0, 0, 0, 0, 0, 0, 82, 9, 0, 0, 0, 0, 95, 0, 0, 0, 0, 0, 0, 26, 98, 0, 0, 0, 0, 20, 102, 0, 0, 0, 0, 0, 0, 56, 73, 0, 0, 0, 0, 39, 86, 0, 0, 0, 0, 1, 32, 114, 40, 0, 0, 0, 0, 38, 103, 51, 63, 83, 91, 89, 55, 121, 4, 0, 0, 0, 0, 0, 36, 44, 44, 19, 0, 0, 33, 107, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 87, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 101, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
        "def test_data_to_verilog(n_bit, data, show_img=0, suffix=''):\n",
        "\n",
        "    # Input data - Flatten into 1-d vector\n",
        "    m_output = data.reshape(1, np.prod(data.shape))\n",
        "    m_output = m_output[0]\n",
        "\n",
        "    # check the output shape is 1D\n",
        "    if m_output.shape[0] != m_output.size:\n",
        "        print('Error: Model output is not 1D. Check the model and layer requested')\n",
        "\n",
        "    # Show the image if requested\n",
        "    if show_img:\n",
        "        plt.subplot(111)\n",
        "        dim=int(np.sqrt(m_output.size))\n",
        "        plt.imshow(m_output.reshape(dim,dim), cmap='Greys')\n",
        "        plt.show()\n",
        "\n",
        "    # Quantize\n",
        "    data_q, data_qi, scale = quantize_nbit(m_output, n_bit)\n",
        "\n",
        "    # Print this arry to verilog\n",
        "    s = \"logic signed [{}:0] test_data{} [0:{}] = '{{ {} }};\".format(n_bit-1, suffix, data_qi.size-1, ', '.join(str(int(e)) for e in data_qi))\n",
        "    print_nowrap(s)\n",
        "\n",
        "    return data_q, data_qi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "oi7TC6l81g1F",
        "outputId": "3ecb604e-2501-446f-e13d-75015f0f22e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.nowrap{white-space:nowrap;}</style><span class='nowrap'>logic signed [7:0] test_data [0:783] = '{ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 116, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 90, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 63, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 76, 105, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 110, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 127, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 111, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 91, 127, 62, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 122, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 99, 127, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 127, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 115, 127, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 127, 60, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 127, 108, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 127, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 43, 89, 124, 127, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 127, 42, 0, 0, 0, 23, 24, 58, 72, 75, 120, 121, 117, 89, 120, 126, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 75, 126, 118, 103, 103, 103, 126, 127, 125, 120, 99, 71, 45, 14, 2, 116, 125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 88, 88, 88, 88, 88, 49, 28, 0, 0, 0, 0, 0, 51, 127, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 127, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 48, 127, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };</span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "test_data, test_data_int = test_data_to_verilog(8,x_train[2],show_img=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-zCqyoY6T6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "12fa2b87cac659b457be56b1f9fd9f19d0e5d67b8aa1ffabf71ca4ccf3ec44db"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}